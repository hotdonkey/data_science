{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /copernicani/status/1721981602753085582 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x136b076d0>, 'Connection to twitter.com timed out. (connect timeout=3.0)'))\n",
      "HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=3.0)\n",
      "HTTPSConnectionPool(host='www.telegraph.co.uk', port=443): Read timed out. (read timeout=3.0)\n",
      "HTTPSConnectionPool(host='www.bbc.com', port=443): Read timed out. (read timeout=3.0)\n",
      "Invalid URL 'item?id=38184195': No scheme supplied. Perhaps you meant https://item?id=38184195?\n",
      "Invalid URL 'item?id=38182965': No scheme supplied. Perhaps you meant https://item?id=38182965?\n",
      "Invalid URL 'item?id=38161369': No scheme supplied. Perhaps you meant https://item?id=38161369?\n",
      "HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /soroushjp/status/1721950722626077067 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x13700b090>, 'Connection to twitter.com timed out. (connect timeout=3.0)'))\n",
      "HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=3.0)\n",
      "Invalid URL 'item?id=38185403': No scheme supplied. Perhaps you meant https://item?id=38185403?\n",
      "HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=3.0)\n",
      "Invalid URL 'item?id=38181383': No scheme supplied. Perhaps you meant https://item?id=38181383?\n",
      "HTTPSConnectionPool(host='iknowwhatyoudownload.com', port=443): Read timed out. (read timeout=3.0)\n",
      "HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /FFmpeg/status/1721275669336707152 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x137207990>, 'Connection to twitter.com timed out. (connect timeout=3.0)'))\n",
      "HTTPSConnectionPool(host='rocumentaries.com', port=443): Read timed out. (read timeout=3.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>points</th>\n",
       "      <th>n_comments</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>load_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh my poor business logic (rednafi.com)</td>\n",
       "      <td>78</td>\n",
       "      <td>32 comments</td>\n",
       "      <td>38159363</td>\n",
       "      <td>https://rednafi.com/misc/oh_my_poor_business_l...</td>\n",
       "      <td>0.201004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northlight technology in Alan Wake 2 (remedyga...</td>\n",
       "      <td>339</td>\n",
       "      <td>161 comments</td>\n",
       "      <td>38180846</td>\n",
       "      <td>https://www.remedygames.com/article/how-northl...</td>\n",
       "      <td>0.411865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go, Containers, and the Linux Scheduler (river...</td>\n",
       "      <td>232</td>\n",
       "      <td>90 comments</td>\n",
       "      <td>38181346</td>\n",
       "      <td>https://www.riverphillips.dev/blog/go-cfs/</td>\n",
       "      <td>0.161152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perfect Dark: Recompiled (hackaday.com)</td>\n",
       "      <td>27</td>\n",
       "      <td>5 comments</td>\n",
       "      <td>38159905</td>\n",
       "      <td>https://hackaday.com/2023/11/05/perfect-dark-r...</td>\n",
       "      <td>0.226509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tumble Forth (hardcoded.net)</td>\n",
       "      <td>71</td>\n",
       "      <td>9 comments</td>\n",
       "      <td>38184539</td>\n",
       "      <td>http://tumbleforth.hardcoded.net/</td>\n",
       "      <td>0.356289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Representations and srategies for games with i...</td>\n",
       "      <td>90</td>\n",
       "      <td>5 comments</td>\n",
       "      <td>38144772</td>\n",
       "      <td>https://www2.cs.sfu.ca/~bbart/personal/masters...</td>\n",
       "      <td>0.7972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Cortextual (cortextual.net)</td>\n",
       "      <td>68</td>\n",
       "      <td>12 comments</td>\n",
       "      <td>38149839</td>\n",
       "      <td>https://cortextual.net/</td>\n",
       "      <td>0.161519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Mass producing the most expensive rice cooker ...</td>\n",
       "      <td>125</td>\n",
       "      <td>225 comments</td>\n",
       "      <td>38142586</td>\n",
       "      <td>https://www.youtube.com/watch?v=xLCwr8qG1p4</td>\n",
       "      <td>0.196764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Curated links to the best documentaries: over ...</td>\n",
       "      <td>17</td>\n",
       "      <td>2 comments</td>\n",
       "      <td>38156039</td>\n",
       "      <td>https://rocumentaries.com/</td>\n",
       "      <td>&gt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Berlin's famed nightclubs, losing customers, f...</td>\n",
       "      <td>152</td>\n",
       "      <td>306 comments</td>\n",
       "      <td>38151205</td>\n",
       "      <td>https://www.npr.org/2023/11/03/1209865472/berl...</td>\n",
       "      <td>0.57633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  news points     n_comments  \\\n",
       "0              Oh my poor business logic (rednafi.com)     78    32 comments   \n",
       "1    Northlight technology in Alan Wake 2 (remedyga...    339   161 comments   \n",
       "2    Go, Containers, and the Linux Scheduler (river...    232    90 comments   \n",
       "3              Perfect Dark: Recompiled (hackaday.com)     27     5 comments   \n",
       "4                         Tumble Forth (hardcoded.net)     71     9 comments   \n",
       "..                                                 ...    ...            ...   \n",
       "295  Representations and srategies for games with i...     90     5 comments   \n",
       "296                        Cortextual (cortextual.net)     68    12 comments   \n",
       "297  Mass producing the most expensive rice cooker ...    125   225 comments   \n",
       "298  Curated links to the best documentaries: over ...     17     2 comments   \n",
       "299  Berlin's famed nightclubs, losing customers, f...    152   306 comments   \n",
       "\n",
       "           id                                               link load_time  \n",
       "0    38159363  https://rednafi.com/misc/oh_my_poor_business_l...  0.201004  \n",
       "1    38180846  https://www.remedygames.com/article/how-northl...  0.411865  \n",
       "2    38181346         https://www.riverphillips.dev/blog/go-cfs/  0.161152  \n",
       "3    38159905  https://hackaday.com/2023/11/05/perfect-dark-r...  0.226509  \n",
       "4    38184539                  http://tumbleforth.hardcoded.net/  0.356289  \n",
       "..        ...                                                ...       ...  \n",
       "295  38144772  https://www2.cs.sfu.ca/~bbart/personal/masters...    0.7972  \n",
       "296  38149839                            https://cortextual.net/  0.161519  \n",
       "297  38142586        https://www.youtube.com/watch?v=xLCwr8qG1p4  0.196764  \n",
       "298  38156039                         https://rocumentaries.com/        >3  \n",
       "299  38151205  https://www.npr.org/2023/11/03/1209865472/berl...   0.57633  \n",
       "\n",
       "[300 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Парсер на основе связки pandas, requests, bs4\n",
    "\n",
    "def hacker_news(pages=10):\n",
    "\n",
    "    news = pd.DataFrame(columns=['news', 'points'])\n",
    "\n",
    "    # Функция расчета времени\n",
    "    def get_load_time(article_url):\n",
    "        # будем ждать 3 секунды, иначе выводить exception и присваивать константное значение\n",
    "        try:\n",
    "\n",
    "            # делаем запрос по url статьи article_url\n",
    "            response = requests.get(\n",
    "                article_url, stream=True, timeout=3.000\n",
    "            )\n",
    "            # получаем время загрузки страницы\n",
    "            load_time = response.elapsed.total_seconds()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            load_time = \">3\"\n",
    "        return load_time\n",
    "\n",
    "    # Функция получения id, news_headline, rating, link\n",
    "    def get_info(page=1):\n",
    "        url = f'https://news.ycombinator.com/?p={page}'\n",
    "\n",
    "        responce = requests.get(url)\n",
    "\n",
    "        # Получаем заголовок\n",
    "        page = pd.read_html(responce.text)[2]\n",
    "        page.drop([1], axis=1, inplace=True)\n",
    "        page.rename(columns={0: 'index', 2: 'news'}, inplace=True)\n",
    "\n",
    "        page = page[(~page['news'].isna()) & (page['news'] != 'More')]\n",
    "\n",
    "        # Получаем рейтинг\n",
    "        points = pd.DataFrame(\n",
    "            page['news'][page['index'].isna()].reset_index(drop=True))\n",
    "        points.rename(columns={'news': 'points'}, inplace=True)\n",
    "\n",
    "        # Проверяем есть ли у новости рейтинг, если его нет то присваиваем ноль\n",
    "        points['contains_points'] = points['points'].str.contains(\"points\")\n",
    "        points['points'] = points['points'].str.split(' ')\n",
    "\n",
    "        def set_points(row):\n",
    "            if row['contains_points']:\n",
    "                return row[0][0]\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        points['points'] = points.apply(set_points, axis=1)\n",
    "        \n",
    "        \n",
    "        # Получаем колличество комментариев\n",
    "        comments = pd.DataFrame(\n",
    "            page['news'][page['index'].isna()].reset_index(drop=True))\n",
    "        comments.rename(columns={'news': 'comments'}, inplace=True)\n",
    "        \n",
    "        comments['n_comments'] = comments['comments'].str.split('|')\n",
    "        comments['contains_comments'] = comments['comments'].str.contains(\"comment*\")\n",
    "        comments['n_comments'] = comments['n_comments'].apply(lambda x: x[-1])\n",
    "\n",
    "\n",
    "        def set_comments(row):\n",
    "            if row['contains_comments']:\n",
    "                return row[1]\n",
    "            else:\n",
    "                return '0 comments'\n",
    "\n",
    "        # Чистим на отсутвие \n",
    "        comments['n_comments'] = comments.apply(set_comments, axis=1)\n",
    "\n",
    "        # Прикручиваем рейтинг к новости\n",
    "        news = pd.DataFrame(\n",
    "            page['news'][~page['index'].isna()].reset_index(drop=True))\n",
    "\n",
    "        result = pd.concat([news, points['points'], comments['n_comments']], axis=1)\n",
    "        \n",
    "        # Начинаем собирать ссылки и id\n",
    "        soup = BeautifulSoup(responce.text, 'html.parser')\n",
    "\n",
    "        link_list = []\n",
    "\n",
    "        for i in range(len(soup.find_all(class_='title'))):\n",
    "            try:\n",
    "                link = soup.find_all(class_='title')[i].find('a')['href']\n",
    "                text = soup.find_all(class_='title')[i].text\n",
    "\n",
    "                if text != 'More':\n",
    "                    link_list.append([text, link])\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Получаем id новостей\n",
    "        trs = soup.find_all('tr', class_='athing')\n",
    "        id_list = []\n",
    "\n",
    "        for tr in trs:\n",
    "            tr_id = tr['id']\n",
    "            id_list.append(tr_id)\n",
    "\n",
    "        id = pd.DataFrame(id_list, columns=['id'])\n",
    "        \n",
    "        # Получаем ссылки новостей\n",
    "        links = pd.DataFrame(link_list)\n",
    "        links = pd.concat([id, links], axis=1)\n",
    "\n",
    "        links.rename(columns={0: 'news', 1: 'link'}, inplace=True)\n",
    "\n",
    "        # Прикручиваем ссылки\n",
    "        result = pd.merge(left=result, right=links)\n",
    "\n",
    "        # Расчитываем время\n",
    "        result['load_time'] = result['link'].apply(get_load_time)\n",
    "\n",
    "        return result\n",
    "\n",
    "    for i in range(pages):\n",
    "        news = pd.concat([news, get_info(i+1)])\n",
    "\n",
    "    news.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return news\n",
    "\n",
    "\n",
    "test = hacker_news(10)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
