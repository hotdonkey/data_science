{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict, OrderedDict, deque\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t\n",
    "from scipy.stats import ttest_ind, shapiro, f_oneway, mannwhitneyu\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from importlib import reload\n",
    "import logging\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding outliers by Tukey method with adjustment option for iqr boundaries.\n",
    "\n",
    "def outliers_iqr(data, feature, left_iqr=1.5, right_iqr=1.5, log_scale=False):\n",
    "    \"\"\"Function for finding outliers by Tukey method with adjustment option for iqr's multiplier.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used to find outliers.\n",
    "        feature (string): Name of a column in DF which will be inspected for outliers.\n",
    "        left_iqr (float, optional): lower boundary multiplier. Defaults to 1.5.\n",
    "        right_iqr (float, optional): upper boundary multiplier. Defaults to 1.5.\n",
    "        log_scale (bool, optional): converting data in logarithmic representation in case of lognormal destribution of\n",
    "        original data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns two copies of the original DataFrame contain DF's with outliers and cleaned data.\n",
    "    \"\"\"    ''''''\n",
    "\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature])\n",
    "    else:\n",
    "        x = data[feature]\n",
    "\n",
    "    quartile1, quartile3 = x.quantile(0.25), x.quantile(0.75)\n",
    "\n",
    "    iqr = quartile3 - quartile1\n",
    "\n",
    "    lower_bound = quartile1 - (iqr*left_iqr)\n",
    "\n",
    "    upper_bound = quartile3 + (iqr*right_iqr)\n",
    "\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "\n",
    "    cleaned = data[(x > lower_bound) & (x < upper_bound)]\n",
    "\n",
    "    print(f'Number of outliers by Tukey\\'s method: {outliers.shape[0]}')\n",
    "    print(f'Resulting number of lines cleared of outliers: {cleaned.shape[0]}')\n",
    "\n",
    "    return outliers, cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding outliers by z-method with adjustment option for boundaries.\n",
    "\n",
    "def outliers_z_score(data, feature, left_mod=3, right_mod=3, log_scale=False):\n",
    "    \"\"\"Function for finding outliers by z-method with adjustment option for left and right multiplier.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used to find outliers.\n",
    "        feature (string): Name of a column in DF which will be inspected for outliers.\n",
    "        left_mod (int, optional): lower boundary multiplier. Defaults to 3.\n",
    "        right_mod (int, optional): upper boundary multiplier. Defaults to 3.\n",
    "        log_scale (bool, optional): converting data in logarithmic representation in case of lognormal destribution of\n",
    "        original data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns two copies of the original DataFrame contain DF's with outliers and cleaned data.\n",
    "    \"\"\"    ''''''\n",
    "\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "\n",
    "    mu = x.mean()\n",
    "\n",
    "    sigma = x.std()\n",
    "\n",
    "    lower_bound = mu - left_mod * sigma\n",
    "\n",
    "    upper_bound = mu + right_mod * sigma\n",
    "\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "\n",
    "    cleaned = data[(x > lower_bound) & (x < upper_bound)]\n",
    "\n",
    "    print(f'Number of outliers by z-method: {outliers.shape[0]}')\n",
    "    print(f'Resulting number of lines cleared of outliers: {cleaned.shape[0]}')\n",
    "\n",
    "    return outliers, cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate finding function in lines\n",
    "\n",
    "def dupl_data_remove(data, immune_col=None):\n",
    "    \"\"\"Function for finding full dupliacates in lines\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used to find duplicates.\n",
    "        immune_col (str or tupple, optional): name of the column/s which the function will pass. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns the copy of the original DataFrame cleaned from full duplicates in line.\n",
    "    \"\"\"    ''''''\n",
    "\n",
    "    if immune_col is None:\n",
    "        dupl_columns = list(data.columns)\n",
    "\n",
    "    else:\n",
    "        dupl_columns = list(data.columns)\n",
    "        try:\n",
    "            for col in immune_col:\n",
    "                dupl_columns.remove(immune_col)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    mask = data.duplicated(subset=dupl_columns)\n",
    "\n",
    "    data_duplicates = data[mask]\n",
    "    print(f'Number of duplicates: {data_duplicates.shape[0]}')\n",
    "\n",
    "    data_dedupped = data.drop_duplicates(subset=dupl_columns)\n",
    "    print(\n",
    "        f'Resulting number of lines cleared of duplicates: {data_dedupped.shape[0]}')\n",
    "\n",
    "    return data_dedupped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate finding function in lines (old version)\n",
    "\n",
    "def dupl_data_remove(data, immune_col=None):\n",
    "    \"\"\"Function for finding full dupliacates in lines\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used to find duplicates.\n",
    "        immune_col (str or tupple, optional): name of the column/s which the function will pass. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns the copy of the original DataFrame cleaned from full duplicates in line.\n",
    "    \"\"\"    ''''''\n",
    "\n",
    "    if immune_col is None:\n",
    "        dupl_columns = list(data.columns)\n",
    "    else:\n",
    "        dupl_columns = list(data.columns)\n",
    "        dupl_columns.remove(immune_col)\n",
    "\n",
    "    mask = data.duplicated(subset=dupl_columns)\n",
    "\n",
    "    data_duplicates = data[mask]\n",
    "    print(f'Number of duplicates: {data_duplicates.shape[0]}')\n",
    "\n",
    "    data_dedupped = data.drop_duplicates(subset=dupl_columns)\n",
    "    print(\n",
    "        f'Resulting number of lines cleared of duplicates: {data_dedupped.shape[0]}')\n",
    "\n",
    "    return data_dedupped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция по поиску неинформативных признаков\n",
    "\n",
    "def low_info_col_drop(data, top_freq_thresh=0.95, nuniq_thresh=0.95):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "        top_freq_thresh (float, optional): _description_. Defaults to 0.95.\n",
    "        nuniq_thresh (float, optional): _description_. Defaults to 0.95.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    ''''''\n",
    "\n",
    "    low_info_col_list = []                                                            #\n",
    "\n",
    "    for col in data.columns:                                                         #\n",
    "        top_freq = data[col].value_counts(\n",
    "            normalize=True).max()                      #\n",
    "        nunique_ratio = data[col].nunique(\n",
    "        ) / data[col].count()                      #\n",
    "\n",
    "    if top_freq > top_freq_thresh:                                                   #\n",
    "        #\n",
    "        low_info_col_list.append(col)\n",
    "        #\n",
    "        print(f'{col}: {round(top_freq*100, 2)}% одинаковых значений')\n",
    "\n",
    "    if nunique_ratio > nuniq_thresh:                                                 #\n",
    "        #\n",
    "        low_info_col_list.append(col)\n",
    "        print(f'{col}: {round(nunique_ratio*100, 2)}% уникальных значений')          #\n",
    "\n",
    "    return low_info_col_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for cleaning data frame from null data features by conditional level of null data.\n",
    "\n",
    "def null_data_col_drop(data, null_thresh=0.4, immune_col=None):\n",
    "    \"\"\"Function for cleaning the data of empty data features.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame which will be used for cleaning.\n",
    "        null_thresh (float, optional): minimal threshold for removing the feature. Defaults to 0.4.\n",
    "        immune_col (str or tupple, optional): name of the column/s which the function will pass. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: returns the copy of the original DataFrame cleaned from null data columns.\n",
    "    \"\"\"    ''''''\n",
    "\n",
    "    # creating the temp frame with replased 0 by NaN\n",
    "    temp_data = data.replace({0: np.nan})\n",
    "    null_data_col_list = []  # empty list for future drop of columns\n",
    "\n",
    "    for col in temp_data.columns:\n",
    "        try:\n",
    "            col_null = round(temp_data[col].isnull().value_counts(normalize=True), 2)[\n",
    "                True]  # percent of NaN in every column\n",
    "        except KeyError:  # if there is no NaN\n",
    "            col_null = 0\n",
    "\n",
    "        if col_null > null_thresh:  # comparison with acceptable level of empty data in column\n",
    "            null_data_col_list.append(col)\n",
    "            print(f'{col}: {col_null*100}% zero values')\n",
    "\n",
    "    if immune_col is None:  # checkin' is there any column/s that the func should skip in drop process\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            for col in immune_col:\n",
    "                null_data_col_list.remove(immune_col)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    drop_data = data.drop(null_data_col_list, axis=1)\n",
    "    print(\n",
    "        f'{drop_data.shape[1]} features with less than {null_thresh*100}% null data')\n",
    "\n",
    "    return drop_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding method on example\n",
    "df = pd.DataFrame({\n",
    "    'Name':['Joe', 'Sue', 'Kirk'],\n",
    "    'Dish':['Shit','Cake','Shit, Cake']\n",
    "})\n",
    "df['Dish_new'] = df['Dish'].str.replace(' ','')\n",
    "df_encoding = df['Dish_new'].str.get_dummies(',')\n",
    "df_final = pd.concat([df, df_encoding], axis=1)\n",
    "df_final.drop(['Dish', 'Dish_new'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доверительные интервалы при известном истинном стандартном отклонении\n",
    "def confidence_interval_sigma(n, x_mean, sigma, gamma=0.95):\n",
    "    \"\"\"Function to evaluate confidence interval with known true standard deviation\n",
    "\n",
    "    Args:\n",
    "        n (int): sample size\n",
    "        x_mean (int): sample mean\n",
    "        sigma (int): true standard deviation\n",
    "        gamma (float): reliability level 0-1\n",
    "\n",
    "    Returns:\n",
    "        float: lower_bound, upper_bound\n",
    "    \"\"\"\n",
    "    alpha = 1 - gamma  # уровень значимости\n",
    "\n",
    "    z_crit = round(-norm.ppf(alpha/2), 2)  # z критическое\n",
    "\n",
    "    eps = z_crit * sigma/(n ** 0.5)  # погрешность\n",
    "    lower_bound = round(x_mean - eps, 2)  # левая (нижняя) граница\n",
    "    upper_bound = round(x_mean + eps, 2)  # правая (верхняя) граница\n",
    "    # создаём кортеж из округлённых границ интервала\n",
    "    confidence_interval = (lower_bound, upper_bound)\n",
    "    # выводим результат\n",
    "    print(\n",
    "        f'Доверительный интервал: {lower_bound, upper_bound}, z-крит: {z_crit}')\n",
    "    return lower_bound, upper_bound, z_crit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доверительные интервалы при неизвестном истинном стандартном отклонении\n",
    "def confidence_interval(n, x_mean, x_std, gamma=0.95):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        n (int): sample size\n",
    "        x_mean (int): sample mean\n",
    "        x_std (int, float): sample standard deviation\n",
    "        gamma (float): reliability level 0-1\n",
    "        \n",
    "    \"\"\"\n",
    "    k = n - 1  # число степеней свободы\n",
    "    alpha = 1 - gamma  # уровень значимости\n",
    "\n",
    "    t_crit = -t.ppf(alpha/2, k)  # t-критическое\n",
    "\n",
    "    eps = t_crit * x_std/(n ** 0.5)  # погрешность\n",
    "    lower_bound = round(x_mean - eps, 2)  # левая (нижняя) граница\n",
    "    upper_bound = round(x_mean + eps, 2)  # правая (верхняя) граница\n",
    "    # создаём кортеж из округлённых границ интервала\n",
    "    confidence_interval = (lower_bound, upper_bound)\n",
    "    #print(\n",
    "    #    f'Доверительный интервал: {lower_bound, upper_bound}, t-крит: {t_crit}')\n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доверительный интервал для пропорции (например конверсии)\n",
    "def proportions_conf_interval(n, x_p, gamma=0.95):\n",
    "    \"\"\"Function to evaluate confidence interval for proportion and\n",
    "    true standard deviation.\n",
    "\n",
    "    Args:\n",
    "        n (int): sample size\n",
    "        x_p (int): sample proportion (% of success)\n",
    "        gamma (float): reliability level 0-1\n",
    "\n",
    "    Returns:\n",
    "        float: lower_bound, upper_bound, sigma\n",
    "    \"\"\"\n",
    "    alpha = 1 - gamma  # уровень значимости\n",
    "    z_crit = -norm.ppf(alpha/2)  # z критическое\n",
    "    eps = z_crit * (x_p * (1 - x_p) / n) ** 0.5  # погрешность\n",
    "    sigma = round((x_p * (1 - x_p))**0.5, 3) #true standard deviation\n",
    "\n",
    "    lower_bound = round((x_p - eps)*100, 2)  # левая (нижняя) граница\n",
    "    upper_bound = round((x_p + eps)*100, 2)  # правая (верхняя) граница\n",
    "\n",
    "    #print(\n",
    "    #    f'Доверительный интервал: {lower_bound, upper_bound}, z-крит: {z_crit}, sigma: {sigma}')\n",
    "    return lower_bound, upper_bound, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доверительный интервал разницы пропорции (например конверсии)\n",
    "def diff_proportions_conf_interval(n, x_p, gamma=0.95):\n",
    "    \"\"\"Function to evaluate confidence interval for delta of proportion.\n",
    "\n",
    "    Args:\n",
    "        n (list): list of samples sizes (for example: n = [a_data['user_id'].count(), b_data['user_id'].count()])\n",
    "        x_p (list): list of mean samples proportions (for example: xp = [a_data['converted'].mean(), b_data['converted'].mean()])\n",
    "        gamma (float, optional): reliability level 0-1\n",
    "\n",
    "    Returns:\n",
    "        float: lower_bound, upper_bound (%)\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = 1 - gamma  # уровень значимости\n",
    "    diff = x_p[1] - x_p[0]  # выборочная разн~ица конверсий групп B и A\n",
    "    z_crit = -norm.ppf(alpha/2)  # z критическое\n",
    "    eps = z_crit * (x_p[0] * (1 - x_p[0])/n[0] + x_p[1] *\n",
    "                    (1 - x_p[1])/n[1]) ** 0.5  # погрешность\n",
    "\n",
    "    lower_bound = round((diff - eps)*100, 2)  # левая (нижняя) граница\n",
    "    upper_bound = round((diff + eps)*100, 2)  # правая (верхняя) граница\n",
    "    # возвращаем кортеж из округлённых границ интервала\n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания лог-файла и записи в него информации\n",
    "def get_logger(path, file):\n",
    "    \"\"\"[Создает лог-файл для логирования в него]\n",
    "    Аргументы:\n",
    "        path {string} -- путь к директории\n",
    "        file {string} -- имя файла\n",
    "    Возвращает:\n",
    "        [obj] -- [логер]\n",
    "    \"\"\"\n",
    "    # проверяем, существует ли файл\n",
    "    log_file = os.path.join(path, file)\n",
    "\n",
    "    # если  файла нет, создаем его\n",
    "    if not os.path.isfile(log_file):\n",
    "        open(log_file, \"w+\").close()\n",
    "\n",
    "    # поменяем формат логирования\n",
    "    file_logging_format = \"%(levelname)s: %(asctime)s: %(message)s\"\n",
    "\n",
    "    # конфигурируем лог-файл\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=file_logging_format)\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    # создадим хэнлдер для записи лога в файл\n",
    "    handler = logging.FileHandler(log_file)\n",
    "\n",
    "    # установим уровень логирования\n",
    "    handler.setLevel(logging.INFO)\n",
    "\n",
    "    # создадим формат логирования, используя file_logging_format\n",
    "    formatter = logging.Formatter(file_logging_format)\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    # добавим хэндлер лог-файлу\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Независимый T-тест\n",
    "def t_test(data, feature_1='', feature_2=''):\n",
    "    \"\"\"T-test\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame\n",
    "        feature_1 (str, optional): Fearure_1.\n",
    "        feature_2 (str, optional): Fearure_2.\n",
    "\n",
    "    Returns:\n",
    "        statistic, p_value\n",
    "    \"\"\"\n",
    "    test_results = ttest_ind(data[feature_1], data[feature_2], equal_var=True)\n",
    "\n",
    "    statistic = round(test_results[0], 4)\n",
    "    p_value = round(test_results[1], 4)\n",
    "\n",
    "    return statistic, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric function print\n",
    "def print_metrics(y_train, y_train_predict, y_test, y_test_predict):\n",
    "    print(f'Train R^2: {round(metrics.r2_score(y_train, y_train_predict),3)}')\n",
    "    print(f'Train MAE: {round(metrics.mean_absolute_error(y_train, y_train_predict),3)}')\n",
    "    print(f'Train MAPE: {round(metrics.mean_absolute_percentage_error(y_train, y_train_predict)*100,2)}')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print(f'Test R^2: {round(metrics.r2_score(y_test, y_test_predict),3)}')\n",
    "    print(f'Test MAE: {round(metrics.mean_absolute_error(y_test, y_test_predict),3)}')\n",
    "    print(f'Test MAPE: {round(metrics.mean_absolute_percentage_error(y_test, y_test_predict)*100,2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled_poly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m lasso_lr_poly \u001b[39m=\u001b[39m linear_model\u001b[39m.\u001b[39mLasso(alpha\u001b[39m=\u001b[39malpha, max_iter\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m#Обучаем модель\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m lasso_lr_poly\u001b[39m.\u001b[39mfit(X_train_scaled_poly, y_train)\n\u001b[1;32m     11\u001b[0m \u001b[39m#Делаем предсказание для тренировочной выборки\u001b[39;00m\n\u001b[1;32m     12\u001b[0m y_train_predict_poly \u001b[39m=\u001b[39m lasso_lr_poly\u001b[39m.\u001b[39mpredict(X_train_scaled_poly)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled_poly' is not defined"
     ]
    }
   ],
   "source": [
    "#Создаём список из 20 возможных значений от 0.001 до 1 L1- регуляризация\n",
    "alpha_list = np.linspace(0.001, 1, 20)\n",
    "#Создаём пустые списки, в которые будем добавлять результаты \n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alpha_list:\n",
    "    #Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "    lasso_lr_poly = linear_model.Lasso(alpha=alpha, max_iter=10000)\n",
    "    #Обучаем модель\n",
    "    lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "    #Делаем предсказание для тренировочной выборки\n",
    "    y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "    #Делаем предсказание для тестовой выборки\n",
    "    y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "    #Рассчитываем коэффициенты детерминации для двух выборок и добавляем их в списки\n",
    "    train_scores.append(metrics.r2_score(y_train, y_train_predict_poly))\n",
    "    test_scores.append(metrics.r2_score(y_test, y_test_predict_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем изменение R^2 в зависимости от alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 4))  # фигура + координатная плоскость\n",
    "# линейный график для тренировочной выборки\n",
    "ax.plot(alpha_list, train_scores, label='Train')\n",
    "# линейный график для тестовой выборки\n",
    "ax.plot(alpha_list, test_scores, label='Test')\n",
    "ax.set_xlabel('Alpha')  # название оси абсцисс\n",
    "ax.set_ylabel('R^2')  # название оси ординат\n",
    "ax.set_xticks(alpha_list)  # метки по оси абсцисс\n",
    "ax.xaxis.set_tick_params(rotation=45)  # поворот меток на оси абсцисс\n",
    "ax.legend()  # отображение легенды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем ошибки\n",
    "fig, axes = plt.subplots(figsize=(12,6))\n",
    "# Ошибки модели на тренировочной выборке\n",
    "y_train_errors = y_train - y_train_pred\n",
    "# Ошибки модели на тестовой выборке\n",
    "y_test_errors = y_test - y_test_pred\n",
    "# Создаем df из ошибок\n",
    "predict_df = pd.DataFrame({\n",
    "    'Train_errors': y_train_errors,\n",
    "    'Test_errors': y_test_errors\n",
    "})\n",
    "# Строим boxplot для ошибок\n",
    "sns.boxplot(data=predict_df, ax= axes)\n",
    "ax.set_xlabel('Model errors')\n",
    "ax.set_ylabel('Model')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём список из 20 возможных значений от 0.001 до 1 L2-регуляризация\n",
    "alpha_list = np.linspace(0.01, 10, 20)\n",
    "#Создаём пустые списки, в которые будем добавлять результаты \n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alpha_list:\n",
    "    #Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "    lasso_lr_poly = linear_model.Ridge(alpha=alpha, max_iter=10000)\n",
    "    #Обучаем модель\n",
    "    lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "    #Делаем предсказание для тренировочной выборки\n",
    "    y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "    #Делаем предсказание для тестовой выборки\n",
    "    y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "    #Рассчитываем коэффициенты детерминации для двух выборок и добавляем их в списки\n",
    "    train_scores.append(metrics.r2_score(y_train, y_train_predict_poly))\n",
    "    test_scores.append(metrics.r2_score(y_test, y_test_predict_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем изменение R^2 в зависимости от alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 4))  # фигура + координатная плоскость\n",
    "# линейный график для тренировочной выборки\n",
    "ax.plot(alpha_list, train_scores, label='Train')\n",
    "# линейный график для тестовой выборки\n",
    "ax.plot(alpha_list, test_scores, label='Test')\n",
    "ax.set_xlabel('Alpha')  # название оси абсцисс\n",
    "ax.set_ylabel('R^2')  # название оси ординат\n",
    "ax.set_xticks(alpha_list)  # метки по оси абсцисс\n",
    "ax.xaxis.set_tick_params(rotation=45)  # поворот меток на оси абсцисс\n",
    "ax.legend()  # отображение легенды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем ошибки\n",
    "fig, axes = plt.subplots(figsize=(12,6))\n",
    "# Ошибки модели на тренировочной выборке\n",
    "y_train_errors = y_train - y_train_pred\n",
    "# Ошибки модели на тестовой выборке\n",
    "y_test_errors = y_test - y_test_pred\n",
    "# Создаем df из ошибок\n",
    "predict_df = pd.DataFrame({\n",
    "    'Train_errors': y_train_errors,\n",
    "    'Test_errors': y_test_errors\n",
    "})\n",
    "# Строим boxplot для ошибок\n",
    "sns.boxplot(data=predict_df, ax= axes)\n",
    "ax.set_xlabel('Model errors')\n",
    "ax.set_ylabel('Model')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция для визуализации модели\n",
    "def plot_probabilities_2d(X, y, model):\n",
    "    #Генерируем координатную сетку из всех возможных значений для признаков\n",
    "    #Glucose изменяется от x1_min = 44 до x2_max = 199, \n",
    "    #BMI — от x2_min = 18.2 до x2_max = 67.1\n",
    "    #Результат работы функции — два массива xx1 и xx2, которые образуют координатную сетку\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(X.iloc[:, 0].min()-1, X.iloc[:, 0].max()+1, 0.1),\n",
    "        np.arange(X.iloc[:, 1].min()-1, X.iloc[:, 1].max()+1, 0.1)\n",
    "    )\n",
    "    #Вытягиваем каждый из массивов в вектор-столбец — reshape(-1, 1)\n",
    "    #Объединяем два столбца в таблицу с помощью hstack\n",
    "    X_net = np.hstack([xx1.reshape(-1, 1), xx2.reshape(-1, 1)])\n",
    "    #Предсказываем вероятность для всех точек на координатной сетке\n",
    "    #Нам нужна только вероятность класса 1\n",
    "    probs = model.predict_proba(X_net)[:, 1]\n",
    "    #Переводим столбец из вероятностей в размер координатной сетки\n",
    "    probs = probs.reshape(xx1.shape)\n",
    "    #Создаём фигуру и координатную плоскость\n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "    #Рисуем тепловую карту вероятностей\n",
    "    contour = ax.contourf(xx1, xx2, probs, 100, cmap='bwr')\n",
    "    #Рисуем разделяющую плоскость — линию, где вероятность равна 0.5\n",
    "    bound = ax.contour(xx1, xx2, probs, [0.5], linewidths=2, colors='black');\n",
    "    #Добавляем цветовую панель \n",
    "    colorbar = fig.colorbar(contour)\n",
    "    #Накладываем поверх тепловой карты диаграмму рассеяния\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y, palette='seismic', ax=ax)\n",
    "    #Даём графику название\n",
    "    ax.set_title('Scatter Plot with Decision Boundary');\n",
    "    #Смещаем легенду в верхний левый угол вне графика\n",
    "    ax.legend(bbox_to_anchor=(-0.05, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция, которая принимает количество кластеров для k-means и матрицу с признаками объектов и возвращает инерцию \n",
    "def get_inertia(cluster_num, X):\n",
    "# инициализируем алгоритм кластеризации\n",
    "    k_means =  KMeans(n_clusters=cluster_num, random_state=42)\n",
    "# запускаем алгоритм k-means\n",
    "    k_means.fit(X)\n",
    "# находим значение инерции\n",
    "    inertia = k_means.inertia_\n",
    "# возвращаем значение инерции\n",
    "    return inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, как и при подсчете метода локтя\n",
    "def get_silhouette(cluster_num, X):\n",
    "    k_means =  KMeans(n_clusters=cluster_num, init='k-means++', n_init=10, random_state=42)\n",
    "    k_means.fit(X)\n",
    "# подсчитаем метрику силуэта, передав данные и то, к каким кластерам относятся объекты\n",
    "    silhouette = silhouette_score(X, k_means.predict(X))\n",
    "    return silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model, X, y, cv, scoring=\"f1\", ax=None, title=\"\"):\n",
    "    # Вычисляем координаты для построения кривой обучения\n",
    "    train_sizes, train_scores, valid_scores = model_selection.learning_curve(\n",
    "        estimator=model,  # модель\n",
    "        X=X,  # матрица наблюдений X\n",
    "        y=y,  # вектор ответов y\n",
    "        cv=cv,  # кросс-валидатор\n",
    "        scoring=scoring,  # метрика\n",
    "    )\n",
    "    # Вычисляем среднее значение по фолдам для каждого набора данных\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "    # Если координатной плоскости не было передано, создаём новую\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))  # фигура + координатная плоскость\n",
    "    # Строим кривую обучения по метрикам на тренировочных фолдах\n",
    "    ax.plot(train_sizes, train_scores_mean, label=\"Train\")\n",
    "    # Строим кривую обучения по метрикам на валидационных фолдах\n",
    "    ax.plot(train_sizes, valid_scores_mean, label=\"Valid\")\n",
    "    # Даём название графику и подписи осям\n",
    "    ax.set_title(\"Learning curve: {}\".format(title))\n",
    "    ax.set_xlabel(\"Train data size\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    # Устанавливаем отметки по оси абсцисс\n",
    "    ax.xaxis.set_ticks(train_sizes)\n",
    "    # Устанавливаем диапазон оси ординат\n",
    "    ax.set_ylim(0, 1)\n",
    "    # Отображаем легенду\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_profile(grouped_data, n_clusters):\n",
    "    # Нормализуем сгруппированные данные, приводя их к масштабу 0-1.\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    grouped_data = pd.DataFrame(scaler.fit_transform(grouped_data), columns=grouped_data.columns)\n",
    "    # Создаем список признаков\n",
    "    features = grouped_data.columns\n",
    "    # Создаем пустую фигуру\n",
    "    fig = go.Figure()\n",
    "    # В цикле визуализируем полярную диаграмму для каждого кластера\n",
    "    for i in range(n_clusters):\n",
    "        # Создаем полярную диаграмму и добавляем ее на общий график\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=grouped_data.iloc[i].values, # радиусы\n",
    "            theta=features, # название засечек\n",
    "            fill='toself', # заливка многоугольника цветом\n",
    "            name=f'Cluster {i}', # название - номер кластера\n",
    "        ))\n",
    "    # Обновляем параметры фигуры\n",
    "    fig.update_layout(\n",
    "        showlegend=True, # отображение легенды\n",
    "        autosize=False, # устаналиваем свои размеры графика\n",
    "        width=800, # ширина (в пикселях)\n",
    "        height=800, # высота (в пикселях)\n",
    "    )\n",
    "    # Отображаем фигуру\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formating date from raw json\n",
    "def date_format(date_raw):\n",
    "    timestamp = date_raw / 1000\n",
    "    date = datetime.datetime.fromtimestamp(timestamp)\n",
    "    formatted_date = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[1, 2, 3]\n",
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Поиск максимальной клики для применения в отборе признаков на основе корреляционной матрицы\n",
    "\n",
    "# Каждая из вершин графа представляет собой отдельный признак,\n",
    "# Ребром соединяются только те вершины, которые не являются сильно коррелированными\n",
    "# при заданном пороге корреляции.\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, vertices):\n",
    "        self.V = vertices\n",
    "        self.adj = [[] for _ in range(vertices)]\n",
    "\n",
    "    def add_edge(self, u, v):\n",
    "        self.adj[u].append(v)\n",
    "        self.adj[v].append(u)\n",
    "\n",
    "    def bron_kerbosch(self, r, p, x):\n",
    "        if len(p) == 0 and len(x) == 0:\n",
    "            print(r)\n",
    "            return r\n",
    "\n",
    "        for v in p[:]:\n",
    "            self.bron_kerbosch(\n",
    "                r + [v], [vertex for vertex in p if vertex in self.adj[v]], \n",
    "                [vertex for vertex in x if vertex in self.adj[v]])\n",
    "            p.remove(v)\n",
    "            x.append(v)\n",
    "\n",
    "    def find_cliques(self):\n",
    "        self.bron_kerbosch([], list(range(self.V)), [])\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g = Graph(5)\n",
    "g.add_edge(0, 1)\n",
    "g.add_edge(0, 2)\n",
    "g.add_edge(1, 2)\n",
    "g.add_edge(1, 3)\n",
    "g.add_edge(2, 3)\n",
    "g.add_edge(3, 4)\n",
    "\n",
    "g.find_cliques()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод отжига\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "def simulated_annealing(initial_state):\n",
    "    \"\"\"Peforms simulated annealing to find a solution\"\"\"\n",
    "    initial_temp = 90\n",
    "    final_temp = .1\n",
    "    alpha = 0.01\n",
    "\n",
    "    current_temp = initial_temp\n",
    "\n",
    "    # Start by initializing the current state with the initial state\n",
    "    current_state = initial_state\n",
    "    solution = current_state\n",
    "\n",
    "    while current_temp > final_temp:\n",
    "        neighbor = random.choice(get_neighbors())\n",
    "\n",
    "        # Check if neighbor is best so far\n",
    "        cost_diff = get_cost(self.current_state) = get_cost(neighbor)\n",
    "\n",
    "        # if the new solution is better, accept it\n",
    "        if cost_diff > 0:\n",
    "            solution = neighbor\n",
    "        # if the new solution is not better, accept it with a probability of e^(-cost/temp)\n",
    "        else:\n",
    "            if random.uniform(0, 1) < math.exp(cost_diff / current_temp):\n",
    "                solution = neighbor\n",
    "        # decrement the temperature\n",
    "        current_temp -= alpha\n",
    "\n",
    "    return solution\n",
    "\n",
    "def get_cost(state):\n",
    "    \"\"\"Calculates cost of the argument state for your solution.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def get_neighbors(state):\n",
    "    \"\"\"Returns neighbors of the argument state for your solution.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average prediction\n",
    "def ma_prediction(train_data, test_data, window_size=3, pred_num=3):\n",
    "    '''Simple function to predict the moving average of a series.'''\n",
    "    data_ma = train_data.rolling(window_size).mean()\n",
    "    last_values = data_ma.tail(window_size).values\n",
    "    prediction = np.array([last_values[-1] +\n",
    "                           (last_values[-1] - last_values[-2]) * i for i in range(1, pred_num + 1)])\n",
    "    prediction = pd.DataFrame(\n",
    "        prediction, columns=test_data.columns, index=test_data.index)\n",
    "    return data_ma, prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qini-score | Qini-curve\n",
    "def qini_df(df, title='train', figsize=(5, 3)):\n",
    "    # Отранжируем выборку по значению uplift в убывающем порядке\n",
    "    ranked = df.sort_values(\"uplift_score\", ascending=False)\n",
    "    \n",
    "    N_c = sum(ranked['target_class'] <= 1)\n",
    "    N_t = sum(ranked['target_class'] >= 2)\n",
    "    \n",
    "    # Посчитаем в отсортированном датафрейме основные показатели, которые используются при расчете qini\n",
    "    ranked['n_c1'] = 0\n",
    "    ranked['n_t1'] = 0\n",
    "    ranked.loc[ranked.target_class == 1,'n_c1'] = 1\n",
    "    ranked.loc[ranked.target_class == 3,'n_t1'] = 1\n",
    "    ranked['n_c1/nc'] = ranked.n_c1.cumsum() / N_c\n",
    "    ranked['n_t1/nt'] = ranked.n_t1.cumsum() / N_t\n",
    "    \n",
    "    # Посчитаем qini curve и рандомную прямую под ней\n",
    "    ranked['uplift'] = round(ranked['n_t1/nt'] - ranked['n_c1/nc'],5)\n",
    "    # Добавим случайную кривую\n",
    "    ranked['random_uplift'] = round(ranked[\"uplift_score\"].rank(pct=True, ascending=False) * ranked['uplift'].iloc[-1],5)\n",
    "    \n",
    "    ranked[\"n\"] = ranked[\"uplift_score\"].rank(pct=True, ascending=False)\n",
    "    \n",
    "    # Немного кода для визуализации\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.plot(ranked['n'], ranked['uplift'], color='r', label='Model')\n",
    "    plt.plot(ranked['n'], ranked['random_uplift'], color='b', label='RandomModel')\n",
    "    plt.legend()\n",
    "    plt.title('Qini-curve for {} samples'.format(title))\n",
    "    plt.show()\n",
    "    quni_score = (ranked['uplift'] - ranked['random_uplift']).sum()\n",
    "    print('Qini score: {:.3f}'.format(quni_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
