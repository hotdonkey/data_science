{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpVt7fst1H8mRe20SDh9yN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Семинар 3 — Создание gRPC-сервиса для ML-модели (protobuf + Python)**\n","\n","## **Цель занятия**\n","\n","Освоить базовые навыки проектирования и реализации gRPC-сервиса для ML-модели:  \n","описать контракт в **Protocol Buffers**, сгенерировать сервер и клиента на Python,  \n","реализовать эндпоинты `/health` и `/predict`, запустить локальные тесты производительности  \n","и подготовить структуру проекта к дальнейшей контейнеризации.\n","\n","---\n","\n","## **План занятия**\n","\n","1. Установка инструментов и подготовка окружения  \n","2. Проектирование контракта API в `.proto`  \n","3. Генерация Python-кода из protobuf  \n","4. Реализация сервера gRPC с загрузкой модели  \n","5. Реализация клиента gRPC и базовых тестов  \n","6. Валидация входных данных, обработка ошибок, таймауты  \n","7. Мини-бенчмарк: сравнение с REST (по желанию)  \n","8. Итог: чек-лист готовности к контейнеризации\n","\n","---\n","\n","## **Предварительные требования**\n","\n","- Python 3.10+  \n","- Виртуальное окружение (venv или conda)  \n","- Пакеты: `grpcio`, `grpcio-tools`, `pandas`, `scikit-learn`, `joblib`, `uvloop` (опционально под Linux)  \n","- Заготовленная модель `model.pkl` (например, LogisticRegression, обученная на Iris или Wine)\n","\n","---\n","\n","## **1. Структура проекта**\n","\n","ml_grpc_service/\n","\n","protos/\n","\n","model.proto\n","\n","server/\n","\n","server.py\n","\n","inference.py\n","\n","validation.py\n","\n","client/\n","\n","client.py\n","\n","models/\n","\n","model.pkl\n","\n","requirements.txt\n","\n","Makefile\n","\n","README.md"],"metadata":{"id":"3qoDhL2LGO0f"}},{"cell_type":"code","source":["# Установка необходимых библиотек\n","!pip install grpcio==1.66.1 grpcio-tools==1.66.1 pandas scikit-learn joblib uvloop -q\n"],"metadata":{"id":"N0ezipPOGPIt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2. Создание структуры проекта**\n","\n","Создадим базовую структуру папок для gRPC-сервиса.\n","\n"],"metadata":{"id":"iDGTpdavGftF"}},{"cell_type":"code","source":["import os\n","\n","folders = [\n","    \"ml_grpc_service/protos\",\n","    \"ml_grpc_service/server\",\n","    \"ml_grpc_service/client\",\n","    \"ml_grpc_service/models\"\n","]\n","for f in folders:\n","    os.makedirs(f, exist_ok=True)\n","\n","print(\"✅ Структура каталогов создана.\")\n","!tree ml_grpc_service\n"],"metadata":{"id":"5gZ-CQkUGnZb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. Проектирование контракта API — `model.proto`**\n","\n","Файл `.proto` описывает, какие методы и типы данных доступны клиенту и серверу.  \n","Ниже — минимальный, но расширяемый контракт с двумя RPC-методами: **Health** и **Predict**.  \n","Добавлено поле `model_version` и возможность передавать произвольные числовые признаки в виде повторяющегося поля.\n"],"metadata":{"id":"D6NzgbKdGpSJ"}},{"cell_type":"code","source":["proto_code = \"\"\"\n","syntax = \"proto3\";\n","package mlservice.v1;\n","\n","service PredictionService {\n","  rpc Health(HealthRequest) returns (HealthResponse);\n","  rpc Predict(PredictRequest) returns (PredictResponse);\n","}\n","\n","message HealthRequest {}\n","\n","message HealthResponse {\n","  string status = 1;         // \"ok\"\n","  string model_version = 2;  // e.g. \"v1.0.3\"\n","}\n","\n","message Feature {\n","  string name = 1;\n","  double value = 2;\n","}\n","\n","message PredictRequest {\n","  repeated Feature features = 1;  // [{name:\"sepal_length\", value:5.1}, ...]\n","}\n","\n","message PredictResponse {\n","  string prediction = 1;          // \"setosa\"\n","  double confidence = 2;          // 0.93 (опционально)\n","  string model_version = 3;\n","}\n","\"\"\"\n","\n","with open(\"ml_grpc_service/protos/model.proto\", \"w\") as f:\n","    f.write(proto_code.strip())\n","\n","print(\"✅ Файл model.proto создан:\")\n","!cat ml_grpc_service/protos/model.proto\n"],"metadata":{"id":"mpynuvQ7Grhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **4. Генерация Python-кода из protobuf**\n","\n","Теперь скомпилируем `.proto` в Python-код.  \n","Команда `protoc` из пакета `grpcio-tools` создаёт два файла:\n","- `model_pb2.py` — описания сообщений;\n","- `model_pb2_grpc.py` — интерфейс сервиса.\n","\n","> ⚠️ Обратите внимание: структура пакетов должна совпадать с путём `mlservice/v1/` в `package` из `.proto`.\n"],"metadata":{"id":"1cxQ4EVvGtrD"}},{"cell_type":"code","source":["!python -m grpc_tools.protoc -I=ml_grpc_service/protos \\\n","  --python_out=ml_grpc_service \\\n","  --grpc_python_out=ml_grpc_service \\\n","  ml_grpc_service/protos/model.proto\n","\n","print(\"✅ Код сгенерирован:\")\n","!ls ml_grpc_service | grep model\n"],"metadata":{"id":"bOWVH1AiGvKW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **5. Логика инференса и валидации данных**\n","\n","### **5.1 inference.py**\n","\n","Файл `server/inference.py` отвечает за загрузку модели и выполнение предсказаний.  \n","Для примера используется модель `model.pkl`, обученная заранее (например, LogisticRegression на Iris).  \n","Метод `predict()` возвращает предсказанный класс и confidence (если доступен `predict_proba`).\n"],"metadata":{"id":"DVi-X1guGxuS"}},{"cell_type":"code","source":["inference_code = \"\"\"\n","import joblib\n","import pandas as pd\n","from pathlib import Path\n","\n","class ModelRunner:\n","    def __init__(self, model_path: str, version: str = \"v1.0.0\"):\n","        self.model = joblib.load(model_path)\n","        self.version = version\n","\n","    def predict(self, features: dict[str, float]) -> tuple[str, float]:\n","        df = pd.DataFrame([features])\n","        y = self.model.predict(df)[0]\n","        try:\n","            proba = float(max(self.model.predict_proba(df)[0]))\n","        except Exception:\n","            proba = 1.0\n","        return str(y), proba\n","\"\"\"\n","\n","with open(\"ml_grpc_service/server/inference.py\", \"w\") as f:\n","    f.write(inference_code.strip())\n","\n","print(\"✅ inference.py создан.\")\n","!head -n 20 ml_grpc_service/server/inference.py\n"],"metadata":{"id":"rMESSI3kG22H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **5.2 validation.py**\n","\n","Файл `server/validation.py` отвечает за проверку входных данных:\n","- отсутствие дубликатов признаков,\n","- отсутствие пустых имён,\n","- наличие хотя бы одного признака.\n","\n","При ошибке выбрасывается исключение `ValidationError`.\n"],"metadata":{"id":"JuxMqDzGG4Uf"}},{"cell_type":"code","source":["validation_code = \"\"\"\n","from typing import Iterable\n","from ml_grpc_service import model_pb2\n","\n","class ValidationError(Exception):\n","    pass\n","\n","def features_to_dict(features: Iterable[model_pb2.Feature]) -> dict[str, float]:\n","    data = {}\n","    for f in features:\n","        if f.name in data:\n","            raise ValidationError(f\"Duplicate feature: {f.name}\")\n","        if not f.name:\n","            raise ValidationError(\"Empty feature name\")\n","        data[f.name] = float(f.value)\n","    if not data:\n","        raise ValidationError(\"No features provided\")\n","    return data\n","\"\"\"\n","\n","with open(\"ml_grpc_service/server/validation.py\", \"w\") as f:\n","    f.write(validation_code.strip())\n","\n","print(\"✅ validation.py создан.\")\n","!head -n 20 ml_grpc_service/server/validation.py\n"],"metadata":{"id":"53J3Cpv5G6Kn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6. Реализация сервера gRPC**\n","\n","Сервер принимает запросы `/Health` и `/Predict`, выполняет валидацию входных данных,  \n","делает предсказание через `ModelRunner` и возвращает результат клиенту.  \n","Ошибки передаются через `context.set_code` и `context.set_details`.\n"],"metadata":{"id":"44jJkQFRG7l1"}},{"cell_type":"code","source":["server_code = \"\"\"\n","import grpc, os\n","from concurrent import futures\n","from ml_grpc_service import model_pb2, model_pb2_grpc\n","from ml_grpc_service.server.inference import ModelRunner\n","from ml_grpc_service.server.validation import features_to_dict, ValidationError\n","\n","MODEL_PATH = os.getenv(\"MODEL_PATH\", \"ml_grpc_service/models/model.pkl\")\n","MODEL_VERSION = os.getenv(\"MODEL_VERSION\", \"v1.0.0\")\n","MAX_WORKERS = int(os.getenv(\"MAX_WORKERS\", \"4\"))\n","PORT = int(os.getenv(\"PORT\", \"50051\"))\n","\n","class PredictionService(model_pb2_grpc.PredictionServiceServicer):\n","    def __init__(self):\n","        self.runner = ModelRunner(MODEL_PATH, version=MODEL_VERSION)\n","\n","    def Health(self, request, context):\n","        return model_pb2.HealthResponse(status=\"ok\", model_version=self.runner.version)\n","\n","    def Predict(self, request, context):\n","        try:\n","            feats = features_to_dict(request.features)\n","            pred, conf = self.runner.predict(feats)\n","            return model_pb2.PredictResponse(\n","                prediction=pred, confidence=conf, model_version=self.runner.version\n","            )\n","        except ValidationError as ve:\n","            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)\n","            context.set_details(str(ve))\n","            return model_pb2.PredictResponse()\n","        except Exception as e:\n","            context.set_code(grpc.StatusCode.INTERNAL)\n","            context.set_details(f\"internal error: {e}\")\n","            return model_pb2.PredictResponse()\n","\n","def serve():\n","    options = [\n","        (\"grpc.max_send_message_length\", 50 * 1024 * 1024),\n","        (\"grpc.max_receive_message_length\", 50 * 1024 * 1024),\n","    ]\n","    server = grpc.server(futures.ThreadPoolExecutor(max_workers=MAX_WORKERS), options=options)\n","    model_pb2_grpc.add_PredictionServiceServicer_to_server(PredictionService(), server)\n","    server.add_insecure_port(f\"[::]:{PORT}\")\n","    server.start()\n","    print(f\"gRPC server started on :{PORT}, model={MODEL_PATH}, version={MODEL_VERSION}\")\n","    server.wait_for_termination()\n","\n","if __name__ == \"__main__\":\n","    try:\n","        import uvloop\n","        uvloop.install()\n","    except Exception:\n","        pass\n","    serve()\n","\"\"\"\n","\n","with open(\"ml_grpc_service/server/server.py\", \"w\") as f:\n","    f.write(server_code.strip())\n","\n","print(\"✅ server.py создан.\")\n","!head -n 25 ml_grpc_service/server/server.py\n"],"metadata":{"id":"HbDverEQG9EJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **7. Клиент gRPC и ручные прогоны**\n","\n","Реализуем клиента, который умеет:\n","- создавать `stub` для подключения к серверу;\n","- вызывать `Health` с таймаутом;\n","- вызывать `Predict` с набором признаков Iris.\n"],"metadata":{"id":"8wyOAgkTG-17"}},{"cell_type":"code","source":["client_code = \"\"\"\n","import grpc\n","from ml_grpc_service import model_pb2, model_pb2_grpc\n","\n","def make_stub(addr: str = \"localhost:50051\"):\n","    channel = grpc.insecure_channel(addr)\n","    return model_pb2_grpc.PredictionServiceStub(channel)\n","\n","def health(stub):\n","    res = stub.Health(model_pb2.HealthRequest(), timeout=2.0)\n","    print(\"Health:\", res.status, \"version:\", res.model_version)\n","\n","def predict(stub):\n","    req = model_pb2.PredictRequest(features=[\n","        model_pb2.Feature(name=\"sepal_length\", value=5.1),\n","        model_pb2.Feature(name=\"sepal_width\", value=3.5),\n","        model_pb2.Feature(name=\"petal_length\", value=1.4),\n","        model_pb2.Feature(name=\"petal_width\", value=0.2),\n","    ])\n","    res = stub.Predict(req, timeout=3.0)\n","    print(\"Prediction:\", res.prediction, \"confidence:\", round(res.confidence, 4), \"version:\", res.model_version)\n","\n","if __name__ == \"__main__\":\n","    stub = make_stub()\n","    health(stub)\n","    predict(stub)\n","\"\"\"\n","with open(\"ml_grpc_service/client/client.py\", \"w\") as f:\n","    f.write(client_code.strip())\n","\n","print(\"✅ client.py создан.\")\n","!sed -n '1,120p' ml_grpc_service/client/client.py\n"],"metadata":{"id":"ajR20tetHIVn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **8. Подготовка модели `model.pkl`**\n","\n","Для демонстрации обучим простую `LogisticRegression` на Iris и сохраним в `ml_grpc_service/models/model.pkl`.\n"],"metadata":{"id":"xfRvoARKHKJ_"}},{"cell_type":"code","source":["from sklearn import datasets\n","from sklearn.linear_model import LogisticRegression\n","import pandas as pd, joblib, os\n","\n","iris = datasets.load_iris(as_frame=True)\n","df = iris.frame.rename(columns={\"target\": \"label\"})\n","X = df.drop(\"label\", axis=1)\n","y = df[\"label\"]\n","\n","model = LogisticRegression(max_iter=500)\n","model.fit(X, y)\n","\n","os.makedirs(\"ml_grpc_service/models\", exist_ok=True)\n","joblib.dump(model, \"ml_grpc_service/models/model.pkl\")\n","print(\"✅ Saved ml_grpc_service/models/model.pkl\")\n"],"metadata":{"id":"iZrg8UWqHOWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **9. Запуск сервера gRPC**\n","\n","Поднимем сервер в фоне и посмотрим его лог.  \n","По умолчанию: `PORT=50051`, `MODEL_PATH=ml_grpc_service/models/model.pkl`.\n"],"metadata":{"id":"pIEFrEIfHPzB"}},{"cell_type":"code","source":["import os, subprocess, time, sys, signal\n","\n","env = os.environ.copy()\n","env[\"PYTHONPATH\"] = os.getcwd()  # чтобы пакеты ml_grpc_service корректно импортировались\n","# при желании можно эмулировать задержку предсказания:\n","# env[\"SLEEP_MS\"] = \"0\"\n","\n","server_proc = subprocess.Popen(\n","    [sys.executable, \"-m\", \"ml_grpc_service.server.server\"],\n","    env=env,\n","    stdout=subprocess.PIPE,\n","    stderr=subprocess.STDOUT,\n","    text=True\n",")\n","\n","time.sleep(1.2)  # дать серверу стартовать\n","\n","# показать первые строки лога\n","for _ in range(4):\n","    line = server_proc.stdout.readline().strip()\n","    if line:\n","        print(line)\n"],"metadata":{"id":"qXpQWj29HRLr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **10. Проверка Health и Predict**\n","\n","Вызовем методы через клиента: `/Health` и `/Predict`.\n"],"metadata":{"id":"LMGhqwy2HTRA"}},{"cell_type":"code","source":["import runpy, os, sys\n","\n","# Запуск клиента как модуля\n","os.environ[\"PYTHONPATH\"] = os.getcwd()\n","runpy.run_path(\"ml_grpc_service/client/client.py\")\n"],"metadata":{"id":"67yHyWENHUeW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **11. Негативные сценарии: валидация и таймауты**\n","\n","1) Попробуем отправить **дубликат признака** — сервер должен вернуть `INVALID_ARGUMENT`.  \n","2) Эмулируем **таймаут** клиента: включим задержку на сервере через переменную окружения `SLEEP_MS`.\n"],"metadata":{"id":"S_og9AXDHXi6"}},{"cell_type":"code","source":["# 11.1 Дубликат признака -> INVALID_ARGUMENT\n","import grpc\n","from ml_grpc_service import model_pb2, model_pb2_grpc\n","\n","stub = model_pb2_grpc.PredictionServiceStub(grpc.insecure_channel(\"localhost:50051\"))\n","\n","bad_req = model_pb2.PredictRequest(features=[\n","    model_pb2.Feature(name=\"sepal_length\", value=5.1),\n","    model_pb2.Feature(name=\"sepal_length\", value=5.2),  # дубликат\n","])\n","try:\n","    _ = stub.Predict(bad_req, timeout=3.0)\n","except grpc.RpcError as e:\n","    print(\"Duplicate feature -> status:\", e.code().name, \"| details:\", e.details())\n","\n","# 11.2 Таймаут клиента: перезапустим сервер с задержкой\n","import os, subprocess, sys, time, signal\n","\n","# остановим текущий сервер\n","server_proc.send_signal(signal.SIGINT)\n","server_proc.wait(timeout=3)\n","\n","env = os.environ.copy()\n","env[\"PYTHONPATH\"] = os.getcwd()\n","env[\"SLEEP_MS\"] = \"500\"  # 0.5 секунды задержка на предсказание\n","\n","server_proc = subprocess.Popen(\n","    [sys.executable, \"-m\", \"ml_grpc_service.server.server\"],\n","    env=env,\n","    stdout=subprocess.PIPE,\n","    stderr=subprocess.STDOUT,\n","    text=True\n",")\n","time.sleep(1.2)\n","\n","# попробуем вызвать с слишком маленьким timeout\n","try:\n","    _ = stub.Predict(\n","        model_pb2.PredictRequest(features=[\n","            model_pb2.Feature(name=\"sepal_length\", value=5.1),\n","            model_pb2.Feature(name=\"sepal_width\", value=3.5),\n","            model_pb2.Feature(name=\"petal_length\", value=1.4),\n","            model_pb2.Feature(name=\"petal_width\", value=0.2),\n","        ]),\n","        timeout=0.2,  # 200 мс < задержки 500 мс\n","    )\n","except grpc.RpcError as e:\n","    print(\"Timeout test -> status:\", e.code().name, \"| details:\", e.details())\n"],"metadata":{"id":"sg3rLd0XHX0D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **12. Мини-бенчмарк: 100 последовательных предсказаний**\n","\n","Сделаем простой прогон 100 RPC-вызовов `Predict` и измерим суммарное время.  \n","(Для корректных результатов установите `SLEEP_MS=0` или не задавайте переменную.)\n"],"metadata":{"id":"BQocRT_uHZO9"}},{"cell_type":"code","source":["# перезапустим сервер без задержки\n","import signal, time, os, sys, subprocess\n","\n","server_proc.send_signal(signal.SIGINT)\n","server_proc.wait(timeout=3)\n","\n","env = os.environ.copy()\n","env[\"PYTHONPATH\"] = os.getcwd()\n","env.pop(\"SLEEP_MS\", None)\n","\n","server_proc = subprocess.Popen(\n","    [sys.executable, \"-m\", \"ml_grpc_service.server.server\"],\n","    env=env,\n","    stdout=subprocess.PIPE,\n","    stderr=subprocess.STDOUT,\n","    text=True\n",")\n","time.sleep(1.2)\n","\n","# бенчмарк\n","import grpc, time\n","from ml_grpc_service import model_pb2, model_pb2_grpc\n","\n","stub = model_pb2_grpc.PredictionServiceStub(grpc.insecure_channel(\"localhost:50051\"))\n","\n","req = model_pb2.PredictRequest(features=[\n","    model_pb2.Feature(name=\"sepal_length\", value=5.1),\n","    model_pb2.Feature(name=\"sepal_width\", value=3.5),\n","    model_pb2.Feature(name=\"petal_length\", value=1.4),\n","    model_pb2.Feature(name=\"petal_width\", value=0.2),\n","])\n","\n","t0 = time.time()\n","N = 100\n","for _ in range(N):\n","    _ = stub.Predict(req, timeout=2.0)\n","t1 = time.time()\n","\n","print(f\"{N} Predict RPCs in {t1 - t0:.3f} s, avg {(t1 - t0)/N*1000:.2f} ms/call\")\n"],"metadata":{"id":"oiUeTTvlHbO0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **13. Обновление сервера для опциональной задержки (для тестов таймаута)**\n","\n","Если вы хотите оставить поддержку переменной `SLEEP_MS`, обновите `server.py`,  \n","добавив внутри `Predict` небольшой `sleep`. Это удобно для учебных тестов таймаутов.\n"],"metadata":{"id":"UMoWi2JiHckx"}},{"cell_type":"code","source":["# Патчим server.py: добавим поддержку задержки через env SLEEP_MS\n","from pathlib import Path\n","\n","p = Path(\"ml_grpc_service/server/server.py\")\n","code = p.read_text()\n","\n","if \"SLEEP_MS\" not in code:\n","    code = code.replace(\n","        \"def Predict(self, request, context):\",\n","        \"def Predict(self, request, context):\\n        import os, time\\n        sleep_ms = int(os.getenv('SLEEP_MS', '0'))\\n        if sleep_ms > 0:\\n            time.sleep(sleep_ms / 1000.0)\"\n","    )\n","    p.write_text(code)\n","\n","print(\"✅ server.py обновлён (SLEEP_MS поддерживается).\")\n","!sed -n '1,140p' ml_grpc_service/server/server.py\n"],"metadata":{"id":"oBrWesbHHd9_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **14. Типовые ошибки и отладка**\n","\n","- **Импорты `model_pb2` не находятся.** Проверьте, куда `protoc` вывел файлы и корректно ли выставлен `PYTHONPATH`.  \n","- **Сервер не видит модель.** Убедитесь, что `MODEL_PATH` указывает на реальный `model.pkl` и текущая рабочая директория верна.  \n","- **Клиент «висит».** Всегда задавайте `timeout` в RPC-вызовах.  \n","- **Изменили `.proto`, а код не обновили.** Регенерируйте Python-модули `model_pb2*.py` на сервере и клиенте.\n"],"metadata":{"id":"piDLJBgVHfjO"}},{"cell_type":"markdown","source":["## **15. Чек-лист готовности к контейнеризации**\n","\n","- Контракт в `.proto` описан и хранится в VCS.  \n","- Python-код сгенерирован автоматикой (`grpcio-tools`) и не редактируется вручную.  \n","- Сервер поднимается, `/Health` возвращает `ok` и `model_version`.  \n","- `/Predict` работает, ошибки валидируются (дубликаты, пустые признаки).  \n","- Клиент использует таймауты и реализует сценарии «счастливый путь» и негативные кейсы.  \n","- Проект структурирован; зависимости закреплены в `requirements.txt`.  \n","- Параметры (`PORT`, `MODEL_PATH`, `MODEL_VERSION`, опционально `SLEEP_MS`) вынесены в переменные окружения.  \n","- Готово к контейнеризации (Dockerfile + `ENTRYPOINT` → следующий семинар)."],"metadata":{"id":"U6rh6Rd3HlSx"}}]}