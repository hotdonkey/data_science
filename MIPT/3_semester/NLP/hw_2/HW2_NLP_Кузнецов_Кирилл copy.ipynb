{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Выбран кодовый формат\n",
        "```\n",
        "\n",
        "# Домашнее задание 2. Поисковая система для документов\n",
        "\n",
        "**Модуль 2. Классический поиск и рекуррентные архитектуры**\n",
        "\n",
        "**ФИО студента: Кузнецов Кирилл Игоревич**\n",
        "\n",
        "**Дата выполнения: 17 сентября 2025**\n",
        "\n",
        "## Описание задания\n",
        "\n",
        "В этом задании вы разработаете полнофункциональную поисковую систему, включающую:\n",
        "1. **Предобработку корпуса.**\n",
        "2. **BM25.**\n",
        "3. **Векторный поиск** — на основе эмбеддингов.\n",
        "4. **Гибридный поиск** — комбинация BM25 и векторного поиска.\n",
        "5. **Выбор метрики и оценку качества** — для конкретной задачи.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Установка и импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# # Установка необходимых библиотек\n",
        "# !pip install sentence-transformers\n",
        "# !pip install faiss-cpu\n",
        "# !pip install rank-bm25  # для сравнения\n",
        "# !pip install pymorphy3 pymorphy3-dicts-ru\n",
        "# !pip install tqdm\n",
        "# !pip install matplotlib seaborn\n",
        "# !pip install pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy3\n",
        "\n",
        "# Векторный поиск\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# BM25\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Создание директорий\n",
        "for dir_name in ['data', 'indices', 'models', 'results', 'tests']:\n",
        "    pathlib.Path(dir_name).mkdir(exist_ok=True)\n",
        "\n",
        "# Загрузка NLTK ресурсов\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph = pymorphy3.MorphAnalyzer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1"
      },
      "source": [
        "## Часть 1. Подготовка данных\n",
        "\n",
        "1. Загрузите и изучите предложенный датасет.  \n",
        "2. Реализуйте функцию предобработки текста, которая включает:\n",
        "- Лемматизацию с использованием pymorphy3.\n",
        "- Удаление стоп-слов и пунктуации.  \n",
        "3. Обработайте весь корпус документов и сохраните результат для последующих шагов.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Загружаем корпус документов\n",
        "ds = load_dataset(\"MLNavigator/russian-retrieval\")\n",
        "df = pd.DataFrame(ds['train'])\n",
        "questions_df = df[['text','q']]\n",
        "\n",
        "\n",
        "# Уберем дубли, так как датасет имеет соответствие много вопросов -> один документ\n",
        "documents = df['text'].drop_duplicates().to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Статистика корпуса:\n",
            "Средняя длина документа: 100.6 слов\n",
            "Медианная длина: 91.0 слов\n",
            "Мин/Макс: 41/1041 слов\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Базовая статистика\n",
        "doc_lengths = [len(doc.split()) for doc in documents]\n",
        "print(f\"\\nСтатистика корпуса:\")\n",
        "print(f\"Средняя длина документа: {np.mean(doc_lengths):.1f} слов\")\n",
        "print(f\"Медианная длина: {np.median(doc_lengths):.1f} слов\")\n",
        "print(f\"Мин/Макс: {min(doc_lengths)}/{max(doc_lengths)} слов\")\n",
        "\n",
        "# Визуализация распределения длин\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(doc_lengths, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "plt.xlabel('Длина документа (слова)')\n",
        "plt.ylabel('Количество документов')\n",
        "plt.title('Распределение длин документов в корпусе')\n",
        "plt.axvline(np.mean(doc_lengths), color='red', linestyle='--', label=f'Среднее: {np.mean(doc_lengths):.1f}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "preprocessing"
      },
      "outputs": [],
      "source": [
        "# 🧹 Предобработка текста - чистим, нормализуем и готовим слова для анализа или модели\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"\n",
        "    Класс для предобработки текста\n",
        "    \n",
        "    Поддерживает:\n",
        "    - Лемматизацию (приведение слов к начальной форме: \"бежали\" → \"бежать\")\n",
        "    - Удаление стоп-слов (типа \"и\", \"в\", \"на\" - они редко несут смысл)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_lemma=True, remove_stopwords=True):\n",
        "        \"\"\"\n",
        "        - use_lemma: приводить слова к начальной форме? (полезно для ML, чтобы \"коты\" и \"кот\" считались одним словом)\n",
        "        - remove_stopwords: удалять бесполезные слова вроде \"это\", \"тот\", \"был\"?\n",
        "\n",
        "        \"\"\"\n",
        "        self.use_lemma = use_lemma\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "\n",
        "        # Загружаем морфологический анализатор для русского языка, для \n",
        "        # опредения части речи, рода, число и привести к нормальной форме\n",
        "        self.morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "        #  Загружаем список стоп-слов на русском (типа \"и\", \"в\", \"что\", \"он\")\n",
        "        self.stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "    def preprocess(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Главный метод\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        # Приводим всё к нижнему регистру\n",
        "        text = text.lower()\n",
        "\n",
        "        # Удаляем HTML-теги и ссылки \n",
        "        text = re.sub(r'<.*?>', ' ', text)      # <b></b> \n",
        "        text = re.sub(r'http\\S+', ' ', text)    # https://\n",
        "        text = re.sub(r'[^а-яёa-z0-9\\s]', ' ', text)  # оставляем только буквы, цифры и пробелы\n",
        "        text = re.sub(r'\\s+', ' ', text)        # несколько пробелов \n",
        "\n",
        "        # 3️⃣ Разбиваем текст на токены по пробелам\n",
        "        tokens = text.split()\n",
        "\n",
        "        # Удаляем стоп-слова\n",
        "        if self.remove_stopwords:\n",
        "            tokens = [t for t in tokens if t not in self.stop_words]\n",
        "\n",
        "        # Приводим слова к нормальной форме (лемматизация)\n",
        "        if self.use_lemma:\n",
        "            # Для каждого слова берём первую (самую вероятную) грамматическую интерпретацию\n",
        "            # и возвращаем её нормальную форму \n",
        "            tokens = [self.morph.parse(t)[0].normal_form for t in tokens]\n",
        "\n",
        "        # Удаляем слишком короткие слова (меньше 3 символов)\n",
        "        tokens = [t for t in tokens if len(t) > 2]\n",
        "\n",
        "        # Возвращаем готовый список чистых, осмысленных слов\n",
        "        return tokens\n",
        "\n",
        "\n",
        "# Создаём экземпляр предобработчика\n",
        "preprocessor = TextPreprocessor(use_lemma=True, remove_stopwords=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Предобработка корпуса...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a92c04edb1ec45fa813bc62150b2d508",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9076 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Размер словаря: 59223 уникальных токенов\n",
            "Средняя длина документа после обработки: 73.9 токенов\n"
          ]
        }
      ],
      "source": [
        "# Предобрабатываем весь корпус\n",
        "print(\"\\nПредобработка корпуса...\")\n",
        "tokenized_docs = [preprocessor.preprocess(doc) for doc in tqdm(documents)]\n",
        "\n",
        "# Статистика после предобработки\n",
        "vocab = set()\n",
        "for doc in tokenized_docs:\n",
        "    vocab.update(doc)\n",
        "\n",
        "print(f\"\\nРазмер словаря: {len(vocab)} уникальных токенов\")\n",
        "print(f\"Средняя длина документа после обработки: {np.mean([len(doc) for doc in tokenized_docs]):.1f} токенов\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2"
      },
      "source": [
        "## Часть 2. Реализация BM25\n",
        "\n",
        "1. Постройте инвертированный индекс для корпуса. Индекс должен содержать частоту термина в документе (TF) и документную частоту (DF).\n",
        "2. Реализуйте функцию поиска BM25 с нуля. Формула для ранжирования:\n",
        "\n",
        "$score(D, Q) = Σ IDF(qi) * (f(qi, D) * (k1 + 1)) / (f(qi, D) + k1 * (1 - b + b * |D| / avgdl))$\n",
        "\n",
        "3. Проведите оптимизацию гиперпараметра k1, чтобы улучшить качество поиска."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Построение инвертированного индекса...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7527956ebad44bbf8218a87cd72ed070",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9076 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Индекс построен: 59223 терминов, 9076 документов\n",
            "Средняя длина документа: 73.86 токенов\n",
            "Статистика:\n",
            "total_terms: 59223\n",
            "total_docs: 9076\n",
            "avg_doc_length: 73.86073159982371\n",
            "avg_posting_length: 9.09006635935363\n",
            "max_posting_length: 4074\n",
            "memory_size_mb: 26.62065887451172\n"
          ]
        }
      ],
      "source": [
        "class InvertedIndex:\n",
        "    \"\"\"Инвертированный индекс для BM25\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = defaultdict(list)   # term -> [(doc_id, term_freq, positions)]\n",
        "        self.doc_lengths = {}            # doc_id -> length\n",
        "        self.doc_count = 0               # общее количество документов\n",
        "        self.avg_doc_length = 0          # средняя длина документа\n",
        "        self.doc_freq = defaultdict(int) # term -> количество документов с термином\n",
        "\n",
        "    def add_document(self, doc_id: int, tokens: List[str]):\n",
        "        \"\"\"Добавление документа в индекс\"\"\"\n",
        "        self.doc_count += 1\n",
        "        self.doc_lengths[doc_id] = len(tokens)\n",
        "\n",
        "        # Подсчёт частот и позиций терминов\n",
        "        term_positions = defaultdict(list)\n",
        "        term_freqs = Counter(tokens)\n",
        "\n",
        "        for pos, term in enumerate(tokens):\n",
        "            term_positions[term].append(pos)\n",
        "\n",
        "        # Добавление в индекс\n",
        "        for term, freq in term_freqs.items():\n",
        "            self.index[term].append({\n",
        "                'doc_id': doc_id,\n",
        "                'freq': freq,\n",
        "                'positions': term_positions[term]\n",
        "            })\n",
        "            self.doc_freq[term] += 1\n",
        "\n",
        "    def build(self, tokenized_docs: List[List[str]]):\n",
        "        \"\"\"Построение индекса для корпуса документов\"\"\"\n",
        "        print(\"Построение инвертированного индекса...\")\n",
        "\n",
        "        for doc_id, tokens in enumerate(tqdm(tokenized_docs)):\n",
        "            self.add_document(doc_id, tokens)\n",
        "\n",
        "        # Вычисляем среднюю длину документа\n",
        "        self.avg_doc_length = sum(self.doc_lengths.values()) / len(self.doc_lengths)\n",
        "\n",
        "        print(f\"Индекс построен: {len(self.index)} терминов, {self.doc_count} документов\")\n",
        "        print(f\"Средняя длина документа: {self.avg_doc_length:.2f} токенов\")\n",
        "\n",
        "    def get_posting_list(self, term: str) -> List[Dict]:\n",
        "        \"\"\"Получение posting list для термина\"\"\"\n",
        "        return self.index.get(term, [])\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Статистика индекса\"\"\"\n",
        "        posting_lengths = [len(postings) for postings in self.index.values()]\n",
        "        return {\n",
        "            'total_terms': len(self.index),\n",
        "            'total_docs': self.doc_count,\n",
        "            'avg_doc_length': self.avg_doc_length,\n",
        "            'avg_posting_length': np.mean(posting_lengths) if posting_lengths else 0,\n",
        "            'max_posting_length': max(posting_lengths) if posting_lengths else 0,\n",
        "            'memory_size_mb': self._estimate_memory() / (1024 * 1024)\n",
        "        }\n",
        "\n",
        "    def _estimate_memory(self) -> int:\n",
        "        \"\"\"Оценка использования памяти в байтах\"\"\"\n",
        "        # Грубая оценка\n",
        "        size = 0\n",
        "        for term, postings in self.index.items():\n",
        "            size += len(term) * 2  # Unicode символы\n",
        "            size += len(postings) * 50  # Примерный размер posting\n",
        "        return size\n",
        "\n",
        "# Создание и построение инвертированного индекса на основе предварительно токенизированных документов\n",
        "index = InvertedIndex()\n",
        "index.build(tokenized_docs)\n",
        "\n",
        "stats = index.get_stats()\n",
        "print(\"Статистика:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BM25:\n",
        "    \"\"\"Реализация алгоритма BM25\"\"\"\n",
        "\n",
        "    def __init__(self, index: InvertedIndex, k1: float = 1.2, b: float = 0.75):\n",
        "        \"\"\"\n",
        "        Параметры:\n",
        "        - k1: параметр насыщения частоты термина (обычно 1.2-2.0)\n",
        "        - b: параметр нормализации длины документа (обычно 0.75)\n",
        "        \"\"\"\n",
        "        self.index = index\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.idf_cache = {}  # Кэш для IDF значений\n",
        "\n",
        "    def _compute_idf(self, term: str) -> float:\n",
        "        \"\"\"Вычисление IDF для термина\"\"\"\n",
        "        if term in self.idf_cache:\n",
        "            return self.idf_cache[term]\n",
        "\n",
        "        N = self.index.doc_count\n",
        "        df = self.index.doc_freq.get(term, 0)\n",
        "\n",
        "        if df == 0:\n",
        "            idf = 0\n",
        "        else:\n",
        "            # Формула IDF для BM25\n",
        "            idf = math.log((N - df + 0.5) / (df + 0.5) + 1)\n",
        "\n",
        "        self.idf_cache[term] = idf\n",
        "        return idf\n",
        "\n",
        "    def score(self, query_tokens: List[str], doc_id: int) -> float:\n",
        "        \"\"\"Вычисление BM25 скора для документа\"\"\"\n",
        "        score = 0.0\n",
        "        doc_length = self.index.doc_lengths.get(doc_id, 0)\n",
        "\n",
        "        if doc_length == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Нормализация длины документа\n",
        "        norm_factor = 1 - self.b + self.b * (doc_length / self.index.avg_doc_length)\n",
        "\n",
        "        # Считаем частоты терминов в документе\n",
        "        doc_term_freqs = defaultdict(int)\n",
        "        for term in query_tokens:\n",
        "            postings = self.index.get_posting_list(term)\n",
        "            for posting in postings:\n",
        "                if posting['doc_id'] == doc_id:\n",
        "                    doc_term_freqs[term] = posting['freq']\n",
        "                    break\n",
        "\n",
        "        # Вычисляем скор\n",
        "        for term in set(query_tokens):  # Уникальные термины запроса\n",
        "            if term not in doc_term_freqs:\n",
        "                continue\n",
        "\n",
        "            idf = self._compute_idf(term)\n",
        "            tf = doc_term_freqs[term]\n",
        "\n",
        "            # Формула BM25\n",
        "            numerator = tf * (self.k1 + 1)\n",
        "            denominator = tf + self.k1 * norm_factor\n",
        "\n",
        "            score += idf * (numerator / denominator)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
        "        \"\"\"Поиск top-k документов для запроса\"\"\"\n",
        "        # Предобработка запроса\n",
        "        query_tokens = preprocessor.preprocess(query)\n",
        "\n",
        "        if not query_tokens:\n",
        "            return []\n",
        "\n",
        "        # Находим документы-кандидаты (содержащие хотя бы один термин)\n",
        "        candidate_docs = set()\n",
        "        for term in query_tokens:\n",
        "            postings = self.index.get_posting_list(term)\n",
        "            for posting in postings:\n",
        "                candidate_docs.add(posting['doc_id'])\n",
        "\n",
        "        # Вычисляем скоры для кандидатов\n",
        "        scores = []\n",
        "        for doc_id in candidate_docs:\n",
        "            score = self.score(query_tokens, doc_id)\n",
        "            if score > 0:\n",
        "                scores.append((doc_id, score))\n",
        "\n",
        "        # Сортируем по скору и возвращаем top-k\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return scores[:top_k]\n",
        "\n",
        "# Создаём экземпляр BM25\n",
        "bm25 = BM25(index, k1=1.2, b=0.75)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Тестирование BM25 поиска на k1=0.8:\n",
            "==================================================\n",
            "\n",
            "Запрос: 'машинное обучение'\n",
            "Топ-3 результата:\n",
            "  Doc 3752 (score: 9.215): Особенность программного обеспечения состоит в том, что оно производится в одной форме — в виде исхо...\n",
            "  Doc 6183 (score: 8.960): Промышленный переворот, произошедший с 60-х годов XVIII до первой четверти XIX веко́в в Великобритан...\n",
            "  Doc 5764 (score: 7.383): Перево́д — деятельность по интерпретации смысла текста на одном языке (исходном языке [ИЯ]) и создан...\n",
            "\n",
            "Запрос: 'нейронные сети'\n",
            "Топ-3 результата:\n",
            "  Doc 7764 (score: 16.253): Области мозга, постоянно используемые, когда человек занят вопросами морали, были исследованы качест...\n",
            "  Doc 6086 (score: 6.005): Двусторонние рынки (двусторонние сети) — сетевые рынки, которые имеют две группы пользователей с воз...\n",
            "  Doc 7568 (score: 5.871): Во множестве сетей пользователи являются гомогенными, то есть они выполняют одинаковые функции. Напр...\n",
            "\n",
            "Запрос: 'поисковые системы BM25'\n",
            "Топ-3 результата:\n",
            "  Doc 4025 (score: 13.496): Для поиска информации с помощью поисковой системы пользователь формулирует поисковый запрос[1]. Рабо...\n",
            "  Doc 4211 (score: 12.944): Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть миллионы веб-страни...\n",
            "  Doc 906 (score: 12.521): Глобальное распространение Интернета и увеличение популярности электронных устройств в арабском и му...\n",
            "\n",
            "Запрос: 'Python программирование'\n",
            "Топ-3 результата:\n",
            "  Doc 1453 (score: 9.361): В теории языков программирования, как подразделе информатики, изучают проектирование, реализацию, ан...\n",
            "  Doc 7576 (score: 8.534): Регулирование (системы регулирования) включают в себя контроль (системы контроля). Функции управлени...\n",
            "  Doc 1437 (score: 8.374): В некоторых университетах информатика преподаётся в качестве теоретического изучения вычислений и ав...\n",
            "\n",
            "Тестирование BM25 поиска на k1=1.0:\n",
            "==================================================\n",
            "\n",
            "Запрос: 'машинное обучение'\n",
            "Топ-3 результата:\n",
            "  Doc 3752 (score: 9.698): Особенность программного обеспечения состоит в том, что оно производится в одной форме — в виде исхо...\n",
            "  Doc 6183 (score: 9.383): Промышленный переворот, произошедший с 60-х годов XVIII до первой четверти XIX веко́в в Великобритан...\n",
            "  Doc 5442 (score: 7.747): Цифровые обучающие игры отличаются от традиционных обучающих игр и не основанного на играх электронн...\n",
            "\n",
            "Запрос: 'нейронные сети'\n",
            "Топ-3 результата:\n",
            "  Doc 7764 (score: 16.936): Области мозга, постоянно используемые, когда человек занят вопросами морали, были исследованы качест...\n",
            "  Doc 6086 (score: 6.523): Двусторонние рынки (двусторонние сети) — сетевые рынки, которые имеют две группы пользователей с воз...\n",
            "  Doc 7568 (score: 6.346): Во множестве сетей пользователи являются гомогенными, то есть они выполняют одинаковые функции. Напр...\n",
            "\n",
            "Запрос: 'поисковые системы BM25'\n",
            "Топ-3 результата:\n",
            "  Doc 4025 (score: 14.591): Для поиска информации с помощью поисковой системы пользователь формулирует поисковый запрос[1]. Рабо...\n",
            "  Doc 4211 (score: 13.871): Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть миллионы веб-страни...\n",
            "  Doc 906 (score: 13.328): Глобальное распространение Интернета и увеличение популярности электронных устройств в арабском и му...\n",
            "\n",
            "Запрос: 'Python программирование'\n",
            "Топ-3 результата:\n",
            "  Doc 1453 (score: 9.982): В теории языков программирования, как подразделе информатики, изучают проектирование, реализацию, ан...\n",
            "  Doc 7576 (score: 8.942): Регулирование (системы регулирования) включают в себя контроль (системы контроля). Функции управлени...\n",
            "  Doc 1437 (score: 8.745): В некоторых университетах информатика преподаётся в качестве теоретического изучения вычислений и ав...\n",
            "\n",
            "Тестирование BM25 поиска на k1=1.2:\n",
            "==================================================\n",
            "\n",
            "Запрос: 'машинное обучение'\n",
            "Топ-3 результата:\n",
            "  Doc 3752 (score: 10.133): Особенность программного обеспечения состоит в том, что оно производится в одной форме — в виде исхо...\n",
            "  Doc 6183 (score: 9.759): Промышленный переворот, произошедший с 60-х годов XVIII до первой четверти XIX веко́в в Великобритан...\n",
            "  Doc 5442 (score: 8.225): Цифровые обучающие игры отличаются от традиционных обучающих игр и не основанного на играх электронн...\n",
            "\n",
            "Запрос: 'нейронные сети'\n",
            "Топ-3 результата:\n",
            "  Doc 7764 (score: 17.544): Области мозга, постоянно используемые, когда человек занят вопросами морали, были исследованы качест...\n",
            "  Doc 6086 (score: 7.018): Двусторонние рынки (двусторонние сети) — сетевые рынки, которые имеют две группы пользователей с воз...\n",
            "  Doc 7568 (score: 6.796): Во множестве сетей пользователи являются гомогенными, то есть они выполняют одинаковые функции. Напр...\n",
            "\n",
            "Запрос: 'поисковые системы BM25'\n",
            "Топ-3 результата:\n",
            "  Doc 4025 (score: 15.628): Для поиска информации с помощью поисковой системы пользователь формулирует поисковый запрос[1]. Рабо...\n",
            "  Doc 4211 (score: 14.736): Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть миллионы веб-страни...\n",
            "  Doc 906 (score: 14.070): Глобальное распространение Интернета и увеличение популярности электронных устройств в арабском и му...\n",
            "\n",
            "Запрос: 'Python программирование'\n",
            "Топ-3 результата:\n",
            "  Doc 1453 (score: 10.554): В теории языков программирования, как подразделе информатики, изучают проектирование, реализацию, ан...\n",
            "  Doc 7576 (score: 9.307): Регулирование (системы регулирования) включают в себя контроль (системы контроля). Функции управлени...\n",
            "  Doc 1437 (score: 9.074): В некоторых университетах информатика преподаётся в качестве теоретического изучения вычислений и ав...\n",
            "\n",
            "Тестирование BM25 поиска на k1=1.5:\n",
            "==================================================\n",
            "\n",
            "Запрос: 'машинное обучение'\n",
            "Топ-3 результата:\n",
            "  Doc 3752 (score: 10.710): Особенность программного обеспечения состоит в том, что оно производится в одной форме — в виде исхо...\n",
            "  Doc 6183 (score: 10.253): Промышленный переворот, произошедший с 60-х годов XVIII до первой четверти XIX веко́в в Великобритан...\n",
            "  Doc 5442 (score: 8.883): Цифровые обучающие игры отличаются от традиционных обучающих игр и не основанного на играх электронн...\n",
            "\n",
            "Запрос: 'нейронные сети'\n",
            "Топ-3 результата:\n",
            "  Doc 7764 (score: 18.344): Области мозга, постоянно используемые, когда человек занят вопросами морали, были исследованы качест...\n",
            "  Doc 6086 (score: 7.721): Двусторонние рынки (двусторонние сети) — сетевые рынки, которые имеют две группы пользователей с воз...\n",
            "  Doc 7568 (score: 7.427): Во множестве сетей пользователи являются гомогенными, то есть они выполняют одинаковые функции. Напр...\n",
            "\n",
            "Запрос: 'поисковые системы BM25'\n",
            "Топ-3 результата:\n",
            "  Doc 4025 (score: 17.088): Для поиска информации с помощью поисковой системы пользователь формулирует поисковый запрос[1]. Рабо...\n",
            "  Doc 4211 (score: 15.929): Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть миллионы веб-страни...\n",
            "  Doc 906 (score: 15.078): Глобальное распространение Интернета и увеличение популярности электронных устройств в арабском и му...\n",
            "\n",
            "Запрос: 'Python программирование'\n",
            "Топ-3 результата:\n",
            "  Doc 1453 (score: 11.334): В теории языков программирования, как подразделе информатики, изучают проектирование, реализацию, ан...\n",
            "  Doc 7576 (score: 9.785): Регулирование (системы регулирования) включают в себя контроль (системы контроля). Функции управлени...\n",
            "  Doc 1437 (score: 9.504): В некоторых университетах информатика преподаётся в качестве теоретического изучения вычислений и ав...\n",
            "\n",
            "Тестирование BM25 поиска на k1=1.8:\n",
            "==================================================\n",
            "\n",
            "Запрос: 'машинное обучение'\n",
            "Топ-3 результата:\n",
            "  Doc 3752 (score: 11.211): Особенность программного обеспечения состоит в том, что оно производится в одной форме — в виде исхо...\n",
            "  Doc 6183 (score: 10.677): Промышленный переворот, произошедший с 60-х годов XVIII до первой четверти XIX веко́в в Великобритан...\n",
            "  Doc 5442 (score: 9.479): Цифровые обучающие игры отличаются от традиционных обучающих игр и не основанного на играх электронн...\n",
            "\n",
            "Запрос: 'нейронные сети'\n",
            "Топ-3 результата:\n",
            "  Doc 7764 (score: 19.033): Области мозга, постоянно используемые, когда человек занят вопросами морали, были исследованы качест...\n",
            "  Doc 6086 (score: 8.380): Двусторонние рынки (двусторонние сети) — сетевые рынки, которые имеют две группы пользователей с воз...\n",
            "  Doc 7568 (score: 8.012): Во множестве сетей пользователи являются гомогенными, то есть они выполняют одинаковые функции. Напр...\n",
            "\n",
            "Запрос: 'поисковые системы BM25'\n",
            "Топ-3 результата:\n",
            "  Doc 4025 (score: 18.442): Для поиска информации с помощью поисковой системы пользователь формулирует поисковый запрос[1]. Рабо...\n",
            "  Doc 4211 (score: 17.012): Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть миллионы веб-страни...\n",
            "  Doc 906 (score: 15.977): Глобальное распространение Интернета и увеличение популярности электронных устройств в арабском и му...\n",
            "\n",
            "Запрос: 'Python программирование'\n",
            "Топ-3 результата:\n",
            "  Doc 1453 (score: 12.033): В теории языков программирования, как подразделе информатики, изучают проектирование, реализацию, ан...\n",
            "  Doc 7576 (score: 10.197): Регулирование (системы регулирования) включают в себя контроль (системы контроля). Функции управлени...\n",
            "  Doc 1437 (score: 9.870): В некоторых университетах информатика преподаётся в качестве теоретического изучения вычислений и ав...\n",
            "\n",
            "Тестирование BM25 поиска на k1=2.0:\n",
            "==================================================\n",
            "\n",
            "Запрос: 'машинное обучение'\n",
            "Топ-3 результата:\n",
            "  Doc 3752 (score: 11.510): Особенность программного обеспечения состоит в том, что оно производится в одной форме — в виде исхо...\n",
            "  Doc 6183 (score: 10.928): Промышленный переворот, произошедший с 60-х годов XVIII до первой четверти XIX веко́в в Великобритан...\n",
            "  Doc 5442 (score: 9.846): Цифровые обучающие игры отличаются от традиционных обучающих игр и не основанного на играх электронн...\n",
            "\n",
            "Запрос: 'нейронные сети'\n",
            "Топ-3 результата:\n",
            "  Doc 7764 (score: 19.442): Области мозга, постоянно используемые, когда человек занят вопросами морали, были исследованы качест...\n",
            "  Doc 6086 (score: 8.798): Двусторонние рынки (двусторонние сети) — сетевые рынки, которые имеют две группы пользователей с воз...\n",
            "  Doc 7568 (score: 8.378): Во множестве сетей пользователи являются гомогенными, то есть они выполняют одинаковые функции. Напр...\n",
            "\n",
            "Запрос: 'поисковые системы BM25'\n",
            "Топ-3 результата:\n",
            "  Doc 4025 (score: 19.292): Для поиска информации с помощью поисковой системы пользователь формулирует поисковый запрос[1]. Рабо...\n",
            "  Doc 4211 (score: 17.680): Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть миллионы веб-страни...\n",
            "  Doc 906 (score: 16.525): Глобальное распространение Интернета и увеличение популярности электронных устройств в арабском и му...\n",
            "\n",
            "Запрос: 'Python программирование'\n",
            "Топ-3 результата:\n",
            "  Doc 1453 (score: 12.460): В теории языков программирования, как подразделе информатики, изучают проектирование, реализацию, ан...\n",
            "  Doc 7576 (score: 10.441): Регулирование (системы регулирования) включают в себя контроль (системы контроля). Функции управлени...\n",
            "  Doc 1437 (score: 10.087): В некоторых университетах информатика преподаётся в качестве теоретического изучения вычислений и ав...\n",
            "\n",
            "Тестирование BM25 поиска на k1=2.5:\n",
            "==================================================\n",
            "\n",
            "Запрос: 'машинное обучение'\n",
            "Топ-3 результата:\n",
            "  Doc 3752 (score: 12.159): Особенность программного обеспечения состоит в том, что оно производится в одной форме — в виде исхо...\n",
            "  Doc 6183 (score: 11.468): Промышленный переворот, произошедший с 60-х годов XVIII до первой четверти XIX веко́в в Великобритан...\n",
            "  Doc 5442 (score: 10.672): Цифровые обучающие игры отличаются от традиционных обучающих игр и не основанного на играх электронн...\n",
            "\n",
            "Запрос: 'нейронные сети'\n",
            "Топ-3 результата:\n",
            "  Doc 7764 (score: 20.327): Области мозга, постоянно используемые, когда человек занят вопросами морали, были исследованы качест...\n",
            "  Doc 6086 (score: 9.771): Двусторонние рынки (двусторонние сети) — сетевые рынки, которые имеют две группы пользователей с воз...\n",
            "  Doc 7568 (score: 9.222): Во множестве сетей пользователи являются гомогенными, то есть они выполняют одинаковые функции. Напр...\n",
            "\n",
            "Запрос: 'поисковые системы BM25'\n",
            "Топ-3 результата:\n",
            "  Doc 4025 (score: 21.252): Для поиска информации с помощью поисковой системы пользователь формулирует поисковый запрос[1]. Рабо...\n",
            "  Doc 4211 (score: 19.190): Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть миллионы веб-страни...\n",
            "  Doc 906 (score: 17.743): Глобальное распространение Интернета и увеличение популярности электронных устройств в арабском и му...\n",
            "\n",
            "Запрос: 'Python программирование'\n",
            "Топ-3 результата:\n",
            "  Doc 1453 (score: 13.411): В теории языков программирования, как подразделе информатики, изучают проектирование, реализацию, ан...\n",
            "  Doc 7576 (score: 10.966): Регулирование (системы регулирования) включают в себя контроль (системы контроля). Функции управлени...\n",
            "  Doc 1437 (score: 10.549): В некоторых университетах информатика преподаётся в качестве теоретического изучения вычислений и ав...\n"
          ]
        }
      ],
      "source": [
        "# Реализация поиска используя BM25\n",
        "k1_candidates = [0.8, 1.0, 1.2, 1.5, 1.8, 2.0, 2.5]\n",
        "\n",
        "\n",
        "def bm25_search_optimiser(k1):\n",
        "\n",
        "    bm25 = BM25(index, k1=k1, b=0.75)\n",
        "\n",
        "    # Тестовые запросы\n",
        "    test_queries = [\n",
        "        \"машинное обучение\",\n",
        "        \"нейронные сети\",\n",
        "        \"поисковые системы BM25\",\n",
        "        \"Python программирование\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nТестирование BM25 поиска на k1={k1}:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for query in test_queries:\n",
        "        results = bm25.search(query, top_k=3)\n",
        "        print(f\"\\nЗапрос: '{query}'\")\n",
        "        print(\"Топ-3 результата:\")\n",
        "\n",
        "        for doc_id, score in results:\n",
        "            # Показываем начало документа\n",
        "            doc_preview = documents[doc_id][:100] + \"...\"\n",
        "            print(f\"  Doc {doc_id} (score: {score:.3f}): {doc_preview}\")\n",
        "            \n",
        "\n",
        "for i in k1_candidates:\n",
        "    bm25_search_optimiser(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Наблюдения:\n",
        "\n",
        "## Скоры монотонно растут с увеличением k1.\n",
        "\n",
        "### Например, для запроса \"нейронные сети\":\n",
        "\n",
        "> k1=0.8 : Doc 7764: 16.253\n",
        "\n",
        "> k1=2.5 : Doc 7764: 20.327\n",
        "\n",
        "* Ранжирование (топ-3) остаётся стабильным.\n",
        "\n",
        "* Для всех запросов топ-3 документов не меняется при изменении k1 - меняются только скоры.\n",
        "\n",
        "* Это говорит о том, что относительная релевантность документов устойчива к k1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3"
      },
      "source": [
        "---\n",
        "---\n",
        "## Часть 3. Векторный поиск\n",
        "\n",
        "1. Используйте предобученную модель sentence-transformers для получения векторных представлений (эмбеддингов) всех документов.\n",
        "2. Создайте индекс для быстрого поиска ближайших соседей с помощью faiss-cpu.\n",
        "3. Реализуйте функцию векторного поиска, которая по запросу находит top-k наиболее близких документов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Тестовые запросы\n",
        "test_queries = [\n",
        "    \"машинное обучение\",\n",
        "    \"нейронные сети\",\n",
        "    \"поисковые системы BM25\",\n",
        "    \"Python программирование\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorSearch:\n",
        "    \"\"\"Векторный поиск с использованием Sentence Transformers и FAISS\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'BAAI/bge-m3'):\n",
        "        print(f\"Загрузка модели {model_name}...\")\n",
        "        self.model = SentenceTransformer(model_name, device='mps')\n",
        "        self.index = None\n",
        "        self.documents = None\n",
        "        self.doc_embeddings = None\n",
        "\n",
        "    def build_index(self, documents: List[str]):\n",
        "        \"\"\"Построение векторного индекса\"\"\"\n",
        "        print(\"Создание эмбеддингов документов...\")\n",
        "        self.documents = documents\n",
        "\n",
        "        # Создаём эмбеддинги для всех документов\n",
        "        self.doc_embeddings = self.model.encode(\n",
        "            documents,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Нормализуем векторы для косинусного сходства\n",
        "        faiss.normalize_L2(self.doc_embeddings)\n",
        "\n",
        "        # Создаём FAISS индекс\n",
        "        dimension = self.doc_embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine similarity после нормализации\n",
        "        self.index.add(self.doc_embeddings)\n",
        "\n",
        "        print(f\"Векторный индекс построен: {len(documents)} документов, размерность {dimension}\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:\n",
        "        \"\"\"Поиск top-k похожих документов\"\"\"\n",
        "        # Создаём эмбеддинг запроса\n",
        "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Поиск в индексе\n",
        "        scores, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Возвращаем результаты в формате (doc_id, score)\n",
        "        results = [(int(idx), float(score)) for idx, score in zip(indices[0], scores[0])]\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загрузка модели BAAI/bge-m3...\n",
            "Создание эмбеддингов документов...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b64ae8227e934f28a827020ba956e87b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/284 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Invalid buffer size: 8.87 GiB",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Создаём векторный поиск\u001b[39;00m\n\u001b[32m      2\u001b[39m vector_search = VectorSearch()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mvector_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mVectorSearch.build_index\u001b[39m\u001b[34m(self, documents)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.documents = documents\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Создаём эмбеддинги для всех документов\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mself\u001b[39m.doc_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Нормализуем векторы для косинусного сходства\u001b[39;00m\n\u001b[32m     24\u001b[39m faiss.normalize_L2(\u001b[38;5;28mself\u001b[39m.doc_embeddings)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1051\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1048\u001b[39m features.update(extra_features)\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1053\u001b[39m         out_features = copy.deepcopy(out_features)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1132\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1126\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1127\u001b[39m         module_kwargs = {\n\u001b[32m   1128\u001b[39m             key: value\n\u001b[32m   1129\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1130\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1131\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:234\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m trans_features = {\n\u001b[32m    229\u001b[39m     key: value\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    232\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    236\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:852\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    850\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    866\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:606\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    602\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    604\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:513\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    512\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    522\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:440\u001b[39m, in \u001b[36mXLMRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    439\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    450\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp_env/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:363\u001b[39m, in \u001b[36mXLMRobertaSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    361\u001b[39m is_causal = \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    373\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
            "\u001b[31mRuntimeError\u001b[39m: Invalid buffer size: 8.87 GiB"
          ]
        }
      ],
      "source": [
        "# Создаём векторный поиск\n",
        "vector_search = VectorSearch()\n",
        "vector_search.build_index(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vector_search"
      },
      "outputs": [],
      "source": [
        "# Тестируем\n",
        "print(\"Тестирование векторного поиска:\")\n",
        "for query in test_queries:\n",
        "    results = vector_search.search(query, top_k=3)\n",
        "    print(f\"Запрос: '{query}'\")\n",
        "    for doc_id, score in results:\n",
        "        doc = documents[doc_id]\n",
        "        print(f\"  [{score:.3f}] {doc.title[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4"
      },
      "source": [
        "___\n",
        "___\n",
        "\n",
        "## Часть 4. Гибридный поиск\n",
        "\n",
        "1. Разработайте функцию, которая комбинирует результаты ранжирования от BM25 и векторного поиска.\n",
        "2. Реализуйте механизм взвешивания скоров с помощью параметра α:\n",
        "hybrid_score = α * bm25_score + (1 - α) * vector_score\n",
        "3. Проведите автоматическую оптимизацию параметра α на валидационном наборе данных.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hybrid_search"
      },
      "outputs": [],
      "source": [
        "# Тестируем гибридный поиск\n",
        "print(\"Тестирование гибридного поиска:\")\n",
        "for query in test_queries:\n",
        "    results = hybrid_search.search(query, top_k=3)\n",
        "    print(f\"Запрос: '{query}'\")\n",
        "    for doc_id, score in results:\n",
        "        doc = documents[doc_id]\n",
        "        print(f\"  [{score:.3f}] {doc.title[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part6"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## Часть 5. Оценка качества\n",
        "\n",
        "1. Выберите и **обоснуйте метрику** для оценки качества вашей поисковой системы (например, MRR, MAP@k или NDCG@k). **Обязательно подумайте о том, какой топ-к нужно выбрать исходя из данных**.\n",
        "2. **Создайте небольшой датасет для оценки**, состоящий из запросов и релевантных им документов.  \n",
        "3. **Сравните качество** всех трех реализованных подходов (BM25, векторный, гибридный) на вашем датасете.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
