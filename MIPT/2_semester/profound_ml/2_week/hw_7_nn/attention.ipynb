{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c563b7",
   "metadata": {},
   "source": [
    "### Задание 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa54a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объединённый словарь:\n",
      "['-', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'H1', 'H2A', 'H2B', 'H3', 'H4', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "# Открываем файл и читаем его содержимое\n",
    "with open('./data/sequences.fasta', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Списки для хранения обработанных данных\n",
    "vocab_aa = set()  # Словарь аминокислот\n",
    "histone_types = set()  # Типы гистонов\n",
    "\n",
    "current_histone_type = None\n",
    "\n",
    "# Обрабатываем каждую строку\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line.startswith('>'):  # Это заголовок последовательности\n",
    "        # Извлекаем тип гистона\n",
    "        parts = line.split('|')\n",
    "        if len(parts) > 2:\n",
    "            histone_type = parts[2]\n",
    "            histone_types.add(histone_type)\n",
    "    else:\n",
    "        # Это строка с последовательностью\n",
    "        vocab_aa.update(line)\n",
    "\n",
    "# Теперь создаём единый словарь\n",
    "vocab = vocab_aa.union(histone_types)\n",
    "\n",
    "# Если нужно, можно преобразовать в обычное множество или список\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Выводим результат\n",
    "print(\"Объединённый словарь:\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecffde4",
   "metadata": {},
   "source": [
    "> Сколько элементов получилось в переменной vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45e8310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195eea1",
   "metadata": {},
   "source": [
    "### Задание 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6648fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество последовательностей в датасете: 565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Создаем char_to_idx и idx_to_char\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "# 2. Перечитываем файл для извлечения последовательностей и типов гистонов\n",
    "sequences = []\n",
    "histone_types_list = []\n",
    "\n",
    "with open('./data/sequences.fasta', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "i = 0\n",
    "while i < len(lines):\n",
    "    line = lines[i].strip()\n",
    "    if line.startswith('>'):\n",
    "        # Извлекаем тип гистона\n",
    "        parts = line.split('|')\n",
    "        histone_type = parts[2]\n",
    "        histone_types_list.append(histone_type)\n",
    "\n",
    "        # Переходим к строке с последовательностью\n",
    "        i += 1\n",
    "        if i < len(lines):\n",
    "            seq = lines[i].strip()\n",
    "            sequences.append(seq)\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# 3. Кодируем последовательности и типы гистонов\n",
    "encoded_sequences = []\n",
    "for seq in sequences:\n",
    "    encoded_seq = [char_to_idx[c] for c in seq]\n",
    "    encoded_sequences.append(torch.tensor(encoded_seq))\n",
    "\n",
    "encoded_types = torch.tensor([char_to_idx[t] for t in histone_types_list])\n",
    "\n",
    "# 4. Результаты\n",
    "num_sequences = len(encoded_sequences)\n",
    "print(f\"Количество последовательностей в датасете: {num_sequences}\")\n",
    "\n",
    "# encoded_sequences - список тензоров (последовательности)\n",
    "# encoded_types - тензор с закодированными типами гистонов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3049d2b",
   "metadata": {},
   "source": [
    "### Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2498e",
   "metadata": {},
   "source": [
    "Для выполнения задания вы можете самостоятельно создать Python-ноутбук, а в LMS прикладывать лишь требуемые результаты. В этом задании вам необходимо написать рекуррентную нейросеть с механизмом внимания с использованием PyTorch. В качестве входного слоя создайте Embedding. Далее создайте 1 рекуррентный слой. В качестве слоя внимания используйте линейную трансформацию. В качестве выходного слоя используйте линейную трансформацию. Используйте код для решения данной задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76b1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 5\n",
    "hidden_dim = 9\n",
    "\n",
    "class RNNWithAttentionModel(nn.Module):\n",
    "   def __init__(self, random_seed=5):\n",
    "       super(RNNWithAttentionModel, self).__init__()\n",
    "       torch.manual_seed(random_seed)\n",
    "       torch.cuda.manual_seed(random_seed)\n",
    "       torch.backends.cudnn.deterministic = True\n",
    "       torch.backends.cudnn.benchmark = False\n",
    "       # Create an embedding layer for the vocabulary\n",
    "       # your code here\n",
    "       # Create an RNN layer\n",
    "       # your code here\n",
    "       # Apply a linear transformation to get the attention scores\n",
    "       # your code here\n",
    "       self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "   def forward(self, x):\n",
    "       x = self.embeddings(x)\n",
    "       out, _ = self.rnn(x)\n",
    "       attention_out = self.attention(out).squeeze(2)\n",
    "       #  Get the attention weights\n",
    "       # your code here\n",
    "       # Compute the context vector\n",
    "       # your code here\n",
    "       out = self.fc(context)\n",
    "       return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8f77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Определяем параметры\n",
    "embedding_dim = 5\n",
    "hidden_dim = 9\n",
    "vocab_size = len(vocab)  # Количество уникальных символов из словаря\n",
    "\n",
    "class RNNWithAttentionModel(nn.Module):\n",
    "    def __init__(self, random_seed=5):\n",
    "        super(RNNWithAttentionModel, self).__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        # Создаем слой встраивания для словаря\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Создаем RNN слой\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Применяем линейную трансформацию для получения оценок внимания\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Выходной слой\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        # Получаем оценки внимания\n",
    "        attention_scores = self.attention(out)  # размер [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Применяем softmax для вычисления весов внимания\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # размер [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Вычисляем контекстный вектор\n",
    "        context = torch.bmm(attention_weights.transpose(1, 2), out)  # размер [batch_size, 1, hidden_dim]\n",
    "        context = context.squeeze(1)  # размер [batch_size, hidden_dim]\n",
    "\n",
    "        # Получаем выход\n",
    "        out = self.fc(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34de9b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/700], Loss: 0.7368, Accuracy: 87.26%\n",
      "Epoch [2/700], Loss: 0.5342, Accuracy: 88.67%\n",
      "Epoch [3/700], Loss: 0.5197, Accuracy: 87.79%\n",
      "Epoch [4/700], Loss: 0.6719, Accuracy: 85.31%\n",
      "Epoch [5/700], Loss: 0.7351, Accuracy: 84.96%\n",
      "Epoch [6/700], Loss: 0.7354, Accuracy: 84.60%\n",
      "Epoch [7/700], Loss: 0.7284, Accuracy: 84.42%\n",
      "Epoch [8/700], Loss: 0.7204, Accuracy: 84.42%\n",
      "Epoch [9/700], Loss: 0.7144, Accuracy: 84.42%\n",
      "Epoch [10/700], Loss: 0.7165, Accuracy: 84.07%\n",
      "Epoch [11/700], Loss: 0.6842, Accuracy: 84.42%\n",
      "Epoch [12/700], Loss: 0.6040, Accuracy: 85.84%\n",
      "Epoch [13/700], Loss: 0.5488, Accuracy: 86.02%\n",
      "Epoch [14/700], Loss: 0.6122, Accuracy: 86.73%\n",
      "Epoch [15/700], Loss: 0.7358, Accuracy: 84.25%\n",
      "Epoch [16/700], Loss: 0.5830, Accuracy: 85.84%\n",
      "Epoch [17/700], Loss: 0.6582, Accuracy: 86.02%\n",
      "Epoch [18/700], Loss: 0.5645, Accuracy: 86.90%\n",
      "Epoch [19/700], Loss: 0.6413, Accuracy: 85.66%\n",
      "Epoch [20/700], Loss: 0.6629, Accuracy: 86.55%\n",
      "Epoch [21/700], Loss: 0.6013, Accuracy: 86.19%\n",
      "Epoch [22/700], Loss: 0.5398, Accuracy: 87.08%\n",
      "Epoch [23/700], Loss: 0.6343, Accuracy: 86.90%\n",
      "Epoch [24/700], Loss: 0.4339, Accuracy: 88.50%\n",
      "Epoch [25/700], Loss: 0.6536, Accuracy: 86.02%\n",
      "Epoch [26/700], Loss: 0.7324, Accuracy: 86.19%\n",
      "Epoch [27/700], Loss: 0.6219, Accuracy: 86.55%\n",
      "Epoch [28/700], Loss: 0.6803, Accuracy: 86.02%\n",
      "Epoch [29/700], Loss: 0.7281, Accuracy: 85.66%\n",
      "Epoch [30/700], Loss: 0.5586, Accuracy: 87.08%\n",
      "Epoch [31/700], Loss: 0.5402, Accuracy: 87.61%\n",
      "Epoch [32/700], Loss: 0.6406, Accuracy: 86.37%\n",
      "Epoch [33/700], Loss: 0.5297, Accuracy: 88.32%\n",
      "Epoch [34/700], Loss: 0.6394, Accuracy: 86.55%\n",
      "Epoch [35/700], Loss: 0.7073, Accuracy: 85.84%\n",
      "Epoch [36/700], Loss: 0.4852, Accuracy: 88.32%\n",
      "Epoch [37/700], Loss: 0.6070, Accuracy: 86.19%\n",
      "Epoch [38/700], Loss: 0.6277, Accuracy: 86.02%\n",
      "Epoch [39/700], Loss: 0.7263, Accuracy: 85.84%\n",
      "Epoch [40/700], Loss: 0.6081, Accuracy: 85.31%\n",
      "Epoch [41/700], Loss: 0.7363, Accuracy: 83.36%\n",
      "Epoch [42/700], Loss: 0.6291, Accuracy: 83.89%\n",
      "Epoch [43/700], Loss: 0.5834, Accuracy: 86.90%\n",
      "Epoch [44/700], Loss: 0.5741, Accuracy: 87.43%\n",
      "Epoch [45/700], Loss: 0.4952, Accuracy: 88.50%\n",
      "Epoch [46/700], Loss: 0.5165, Accuracy: 88.32%\n",
      "Epoch [47/700], Loss: 0.7312, Accuracy: 85.31%\n",
      "Epoch [48/700], Loss: 0.7250, Accuracy: 85.49%\n",
      "Epoch [49/700], Loss: 0.5065, Accuracy: 86.90%\n",
      "Epoch [50/700], Loss: 0.6212, Accuracy: 87.43%\n",
      "Epoch [51/700], Loss: 0.6324, Accuracy: 86.37%\n",
      "Epoch [52/700], Loss: 0.6371, Accuracy: 86.90%\n",
      "Epoch [53/700], Loss: 0.6782, Accuracy: 85.84%\n",
      "Epoch [54/700], Loss: 0.7157, Accuracy: 84.78%\n",
      "Epoch [55/700], Loss: 0.5237, Accuracy: 87.96%\n",
      "Epoch [56/700], Loss: 0.6943, Accuracy: 85.49%\n",
      "Epoch [57/700], Loss: 0.5010, Accuracy: 87.96%\n",
      "Epoch [58/700], Loss: 0.6215, Accuracy: 86.37%\n",
      "Epoch [59/700], Loss: 0.6256, Accuracy: 86.73%\n",
      "Epoch [60/700], Loss: 0.6822, Accuracy: 85.84%\n",
      "Epoch [61/700], Loss: 0.6436, Accuracy: 86.19%\n",
      "Epoch [62/700], Loss: 0.7675, Accuracy: 84.25%\n",
      "Epoch [63/700], Loss: 0.6838, Accuracy: 85.13%\n",
      "Epoch [64/700], Loss: 0.6155, Accuracy: 84.25%\n",
      "Epoch [65/700], Loss: 0.5818, Accuracy: 85.13%\n",
      "Epoch [66/700], Loss: 0.5743, Accuracy: 87.26%\n",
      "Epoch [67/700], Loss: 0.5606, Accuracy: 87.43%\n",
      "Epoch [68/700], Loss: 0.5293, Accuracy: 87.08%\n",
      "Epoch [69/700], Loss: 0.6809, Accuracy: 83.89%\n",
      "Epoch [70/700], Loss: 0.6722, Accuracy: 84.25%\n",
      "Epoch [71/700], Loss: 0.5024, Accuracy: 87.96%\n",
      "Epoch [72/700], Loss: 0.6246, Accuracy: 85.13%\n",
      "Epoch [73/700], Loss: 0.6533, Accuracy: 83.72%\n",
      "Epoch [74/700], Loss: 0.5262, Accuracy: 87.26%\n",
      "Epoch [75/700], Loss: 0.6239, Accuracy: 86.73%\n",
      "Epoch [76/700], Loss: 0.6586, Accuracy: 84.96%\n",
      "Epoch [77/700], Loss: 0.6768, Accuracy: 85.49%\n",
      "Epoch [78/700], Loss: 0.6738, Accuracy: 84.96%\n",
      "Epoch [79/700], Loss: 0.6425, Accuracy: 87.43%\n",
      "Epoch [80/700], Loss: 0.5201, Accuracy: 87.26%\n",
      "Epoch [81/700], Loss: 0.6956, Accuracy: 84.78%\n",
      "Epoch [82/700], Loss: 0.8424, Accuracy: 85.31%\n",
      "Epoch [83/700], Loss: 0.5562, Accuracy: 86.02%\n",
      "Epoch [84/700], Loss: 0.6041, Accuracy: 87.61%\n",
      "Epoch [85/700], Loss: 0.7104, Accuracy: 85.49%\n",
      "Epoch [86/700], Loss: 0.5206, Accuracy: 86.90%\n",
      "Epoch [87/700], Loss: 0.6105, Accuracy: 85.66%\n",
      "Epoch [88/700], Loss: 0.7025, Accuracy: 83.01%\n",
      "Epoch [89/700], Loss: 0.5539, Accuracy: 86.37%\n",
      "Epoch [90/700], Loss: 0.5706, Accuracy: 84.96%\n",
      "Epoch [91/700], Loss: 0.6145, Accuracy: 84.25%\n",
      "Epoch [92/700], Loss: 0.6574, Accuracy: 84.96%\n",
      "Epoch [93/700], Loss: 0.5825, Accuracy: 87.08%\n",
      "Epoch [94/700], Loss: 0.5582, Accuracy: 87.43%\n",
      "Epoch [95/700], Loss: 0.6081, Accuracy: 86.02%\n",
      "Epoch [96/700], Loss: 0.6487, Accuracy: 86.73%\n",
      "Epoch [97/700], Loss: 0.5022, Accuracy: 86.55%\n",
      "Epoch [98/700], Loss: 0.4933, Accuracy: 87.79%\n",
      "Epoch [99/700], Loss: 0.6814, Accuracy: 86.19%\n",
      "Epoch [100/700], Loss: 0.6396, Accuracy: 87.08%\n",
      "Epoch [101/700], Loss: 0.5727, Accuracy: 86.02%\n",
      "Epoch [102/700], Loss: 0.5673, Accuracy: 85.84%\n",
      "Epoch [103/700], Loss: 0.5906, Accuracy: 87.79%\n",
      "Epoch [104/700], Loss: 0.6604, Accuracy: 86.19%\n",
      "Epoch [105/700], Loss: 0.6027, Accuracy: 86.90%\n",
      "Epoch [106/700], Loss: 0.6258, Accuracy: 86.37%\n",
      "Epoch [107/700], Loss: 0.6810, Accuracy: 85.84%\n",
      "Epoch [108/700], Loss: 0.7074, Accuracy: 84.78%\n",
      "Epoch [109/700], Loss: 0.6751, Accuracy: 83.89%\n",
      "Epoch [110/700], Loss: 0.5250, Accuracy: 87.08%\n",
      "Epoch [111/700], Loss: 0.6590, Accuracy: 86.02%\n",
      "Epoch [112/700], Loss: 0.7200, Accuracy: 84.78%\n",
      "Epoch [113/700], Loss: 0.6237, Accuracy: 86.02%\n",
      "Epoch [114/700], Loss: 0.7184, Accuracy: 84.78%\n",
      "Epoch [115/700], Loss: 0.5912, Accuracy: 86.90%\n",
      "Epoch [116/700], Loss: 0.7372, Accuracy: 84.42%\n",
      "Epoch [117/700], Loss: 0.6199, Accuracy: 85.66%\n",
      "Epoch [118/700], Loss: 0.5917, Accuracy: 86.19%\n",
      "Epoch [119/700], Loss: 0.6433, Accuracy: 86.02%\n",
      "Epoch [120/700], Loss: 0.6563, Accuracy: 84.96%\n",
      "Epoch [121/700], Loss: 0.7029, Accuracy: 85.66%\n",
      "Epoch [122/700], Loss: 0.6171, Accuracy: 86.90%\n",
      "Epoch [123/700], Loss: 0.7584, Accuracy: 84.96%\n",
      "Epoch [124/700], Loss: 0.7351, Accuracy: 84.78%\n",
      "Epoch [125/700], Loss: 0.7217, Accuracy: 84.96%\n",
      "Epoch [126/700], Loss: 0.7188, Accuracy: 84.96%\n",
      "Epoch [127/700], Loss: 0.7164, Accuracy: 84.96%\n",
      "Epoch [128/700], Loss: 0.6401, Accuracy: 85.31%\n",
      "Epoch [129/700], Loss: 0.5792, Accuracy: 87.26%\n",
      "Epoch [130/700], Loss: 0.5777, Accuracy: 86.02%\n",
      "Epoch [131/700], Loss: 0.5932, Accuracy: 85.84%\n",
      "Epoch [132/700], Loss: 0.5950, Accuracy: 86.73%\n",
      "Epoch [133/700], Loss: 0.7983, Accuracy: 83.19%\n",
      "Epoch [134/700], Loss: 0.5671, Accuracy: 86.73%\n",
      "Epoch [135/700], Loss: 0.5895, Accuracy: 86.02%\n",
      "Epoch [136/700], Loss: 0.5743, Accuracy: 87.26%\n",
      "Epoch [137/700], Loss: 0.5423, Accuracy: 87.79%\n",
      "Epoch [138/700], Loss: 0.6092, Accuracy: 86.73%\n",
      "Epoch [139/700], Loss: 0.8169, Accuracy: 84.60%\n",
      "Epoch [140/700], Loss: 0.6791, Accuracy: 84.78%\n",
      "Epoch [141/700], Loss: 0.6328, Accuracy: 86.55%\n",
      "Epoch [142/700], Loss: 0.6244, Accuracy: 85.66%\n",
      "Epoch [143/700], Loss: 0.6381, Accuracy: 85.84%\n",
      "Epoch [144/700], Loss: 0.6571, Accuracy: 85.84%\n",
      "Epoch [145/700], Loss: 0.4255, Accuracy: 88.67%\n",
      "Epoch [146/700], Loss: 0.6626, Accuracy: 86.19%\n",
      "Epoch [147/700], Loss: 0.5929, Accuracy: 86.73%\n",
      "Epoch [148/700], Loss: 0.5978, Accuracy: 87.08%\n",
      "Epoch [149/700], Loss: 0.6821, Accuracy: 86.37%\n",
      "Epoch [150/700], Loss: 0.7243, Accuracy: 84.42%\n",
      "Epoch [151/700], Loss: 0.6187, Accuracy: 85.49%\n",
      "Epoch [152/700], Loss: 0.5898, Accuracy: 87.08%\n",
      "Epoch [153/700], Loss: 0.5944, Accuracy: 87.26%\n",
      "Epoch [154/700], Loss: 0.6196, Accuracy: 86.55%\n",
      "Epoch [155/700], Loss: 0.8482, Accuracy: 80.53%\n",
      "Epoch [156/700], Loss: 0.7934, Accuracy: 83.01%\n",
      "Epoch [157/700], Loss: 0.7232, Accuracy: 83.89%\n",
      "Epoch [158/700], Loss: 0.6982, Accuracy: 84.60%\n",
      "Epoch [159/700], Loss: 0.7372, Accuracy: 84.42%\n",
      "Epoch [160/700], Loss: 0.6810, Accuracy: 85.13%\n",
      "Epoch [161/700], Loss: 0.6988, Accuracy: 84.96%\n",
      "Epoch [162/700], Loss: 0.7779, Accuracy: 84.25%\n",
      "Epoch [163/700], Loss: 0.6849, Accuracy: 84.78%\n",
      "Epoch [164/700], Loss: 0.7011, Accuracy: 84.60%\n",
      "Epoch [165/700], Loss: 0.6607, Accuracy: 85.31%\n",
      "Epoch [166/700], Loss: 0.6938, Accuracy: 84.78%\n",
      "Epoch [167/700], Loss: 0.6528, Accuracy: 85.13%\n",
      "Epoch [168/700], Loss: 0.5320, Accuracy: 86.37%\n",
      "Epoch [169/700], Loss: 0.7688, Accuracy: 83.72%\n",
      "Epoch [170/700], Loss: 0.6306, Accuracy: 84.96%\n",
      "Epoch [171/700], Loss: 0.6899, Accuracy: 84.07%\n",
      "Epoch [172/700], Loss: 0.6247, Accuracy: 85.66%\n",
      "Epoch [173/700], Loss: 0.6505, Accuracy: 84.42%\n",
      "Epoch [174/700], Loss: 0.6508, Accuracy: 84.78%\n",
      "Epoch [175/700], Loss: 0.6199, Accuracy: 84.96%\n",
      "Epoch [176/700], Loss: 0.6670, Accuracy: 84.96%\n",
      "Epoch [177/700], Loss: 0.5791, Accuracy: 86.37%\n",
      "Epoch [178/700], Loss: 0.6594, Accuracy: 85.84%\n",
      "Epoch [179/700], Loss: 0.6868, Accuracy: 85.13%\n",
      "Epoch [180/700], Loss: 0.6566, Accuracy: 84.42%\n",
      "Epoch [181/700], Loss: 0.5666, Accuracy: 85.49%\n",
      "Epoch [182/700], Loss: 0.6537, Accuracy: 85.84%\n",
      "Epoch [183/700], Loss: 0.7623, Accuracy: 84.60%\n",
      "Epoch [184/700], Loss: 0.5777, Accuracy: 86.19%\n",
      "Epoch [185/700], Loss: 0.7291, Accuracy: 83.89%\n",
      "Epoch [186/700], Loss: 0.6311, Accuracy: 85.31%\n",
      "Epoch [187/700], Loss: 0.6336, Accuracy: 86.37%\n",
      "Epoch [188/700], Loss: 0.6364, Accuracy: 86.37%\n",
      "Epoch [189/700], Loss: 0.7294, Accuracy: 85.13%\n",
      "Epoch [190/700], Loss: 0.7235, Accuracy: 84.96%\n",
      "Epoch [191/700], Loss: 0.7177, Accuracy: 84.96%\n",
      "Epoch [192/700], Loss: 0.7127, Accuracy: 84.96%\n",
      "Epoch [193/700], Loss: 0.7081, Accuracy: 84.96%\n",
      "Epoch [194/700], Loss: 0.7071, Accuracy: 84.96%\n",
      "Epoch [195/700], Loss: 0.7058, Accuracy: 84.96%\n",
      "Epoch [196/700], Loss: 0.7053, Accuracy: 84.96%\n",
      "Epoch [197/700], Loss: 0.7056, Accuracy: 84.96%\n",
      "Epoch [198/700], Loss: 0.7067, Accuracy: 84.96%\n",
      "Epoch [199/700], Loss: 0.7058, Accuracy: 84.96%\n",
      "Epoch [200/700], Loss: 0.7115, Accuracy: 84.78%\n",
      "Epoch [201/700], Loss: 0.7025, Accuracy: 84.25%\n",
      "Epoch [202/700], Loss: 0.6610, Accuracy: 84.60%\n",
      "Epoch [203/700], Loss: 0.6798, Accuracy: 84.78%\n",
      "Epoch [204/700], Loss: 0.6597, Accuracy: 84.78%\n",
      "Epoch [205/700], Loss: 0.6517, Accuracy: 85.31%\n",
      "Epoch [206/700], Loss: 0.7204, Accuracy: 84.25%\n",
      "Epoch [207/700], Loss: 0.5747, Accuracy: 86.19%\n",
      "Epoch [208/700], Loss: 0.7106, Accuracy: 85.13%\n",
      "Epoch [209/700], Loss: 0.6653, Accuracy: 84.78%\n",
      "Epoch [210/700], Loss: 0.6267, Accuracy: 85.49%\n",
      "Epoch [211/700], Loss: 0.7103, Accuracy: 84.60%\n",
      "Epoch [212/700], Loss: 0.6163, Accuracy: 85.49%\n",
      "Epoch [213/700], Loss: 0.6257, Accuracy: 85.31%\n",
      "Epoch [214/700], Loss: 0.6962, Accuracy: 85.13%\n",
      "Epoch [215/700], Loss: 0.6626, Accuracy: 84.96%\n",
      "Epoch [216/700], Loss: 0.6310, Accuracy: 85.31%\n",
      "Epoch [217/700], Loss: 0.6546, Accuracy: 85.66%\n",
      "Epoch [218/700], Loss: 0.7450, Accuracy: 83.72%\n",
      "Epoch [219/700], Loss: 0.6173, Accuracy: 85.31%\n",
      "Epoch [220/700], Loss: 0.6758, Accuracy: 85.31%\n",
      "Epoch [221/700], Loss: 0.7489, Accuracy: 84.42%\n",
      "Epoch [222/700], Loss: 0.6752, Accuracy: 85.13%\n",
      "Epoch [223/700], Loss: 0.5667, Accuracy: 86.55%\n",
      "Epoch [224/700], Loss: 0.7270, Accuracy: 85.13%\n",
      "Epoch [225/700], Loss: 0.7523, Accuracy: 84.96%\n",
      "Epoch [226/700], Loss: 0.7316, Accuracy: 84.78%\n",
      "Epoch [227/700], Loss: 0.5974, Accuracy: 86.02%\n",
      "Epoch [228/700], Loss: 0.9008, Accuracy: 83.36%\n",
      "Epoch [229/700], Loss: 0.6808, Accuracy: 84.96%\n",
      "Epoch [230/700], Loss: 0.6532, Accuracy: 84.60%\n",
      "Epoch [231/700], Loss: 0.6573, Accuracy: 85.66%\n",
      "Epoch [232/700], Loss: 0.6688, Accuracy: 85.13%\n",
      "Epoch [233/700], Loss: 0.6338, Accuracy: 86.19%\n",
      "Epoch [234/700], Loss: 0.6502, Accuracy: 86.55%\n",
      "Epoch [235/700], Loss: 0.5187, Accuracy: 87.96%\n",
      "Epoch [236/700], Loss: 0.6573, Accuracy: 86.19%\n",
      "Epoch [237/700], Loss: 0.7484, Accuracy: 84.42%\n",
      "Epoch [238/700], Loss: 0.6147, Accuracy: 85.49%\n",
      "Epoch [239/700], Loss: 0.6367, Accuracy: 84.60%\n",
      "Epoch [240/700], Loss: 0.6686, Accuracy: 85.84%\n",
      "Epoch [241/700], Loss: 0.8042, Accuracy: 84.07%\n",
      "Epoch [242/700], Loss: 0.6164, Accuracy: 86.02%\n",
      "Epoch [243/700], Loss: 0.7376, Accuracy: 83.19%\n",
      "Epoch [244/700], Loss: 0.6141, Accuracy: 85.13%\n",
      "Epoch [245/700], Loss: 0.6365, Accuracy: 86.73%\n",
      "Epoch [246/700], Loss: 0.7322, Accuracy: 84.78%\n",
      "Epoch [247/700], Loss: 0.7150, Accuracy: 84.60%\n",
      "Epoch [248/700], Loss: 0.5289, Accuracy: 87.43%\n",
      "Epoch [249/700], Loss: 0.7541, Accuracy: 84.96%\n",
      "Epoch [250/700], Loss: 0.6207, Accuracy: 84.25%\n",
      "Epoch [251/700], Loss: 0.5818, Accuracy: 86.02%\n",
      "Epoch [252/700], Loss: 0.6502, Accuracy: 87.08%\n",
      "Epoch [253/700], Loss: 0.5189, Accuracy: 87.43%\n",
      "Epoch [254/700], Loss: 0.6871, Accuracy: 85.31%\n",
      "Epoch [255/700], Loss: 0.5769, Accuracy: 86.73%\n",
      "Epoch [256/700], Loss: 0.6557, Accuracy: 85.31%\n",
      "Epoch [257/700], Loss: 0.5634, Accuracy: 86.90%\n",
      "Epoch [258/700], Loss: 0.7315, Accuracy: 84.42%\n",
      "Epoch [259/700], Loss: 0.5398, Accuracy: 86.73%\n",
      "Epoch [260/700], Loss: 0.5907, Accuracy: 86.73%\n",
      "Epoch [261/700], Loss: 0.6279, Accuracy: 86.55%\n",
      "Epoch [262/700], Loss: 0.7548, Accuracy: 84.42%\n",
      "Epoch [263/700], Loss: 0.6770, Accuracy: 84.96%\n",
      "Epoch [264/700], Loss: 0.5940, Accuracy: 86.73%\n",
      "Epoch [265/700], Loss: 0.6289, Accuracy: 85.84%\n",
      "Epoch [266/700], Loss: 0.6630, Accuracy: 85.84%\n",
      "Epoch [267/700], Loss: 0.6448, Accuracy: 85.84%\n",
      "Epoch [268/700], Loss: 0.7274, Accuracy: 83.89%\n",
      "Epoch [269/700], Loss: 0.6546, Accuracy: 83.54%\n",
      "Epoch [270/700], Loss: 0.6454, Accuracy: 85.31%\n",
      "Epoch [271/700], Loss: 0.7149, Accuracy: 84.96%\n",
      "Epoch [272/700], Loss: 0.6624, Accuracy: 84.96%\n",
      "Epoch [273/700], Loss: 0.5255, Accuracy: 87.61%\n",
      "Epoch [274/700], Loss: 0.7078, Accuracy: 85.66%\n",
      "Epoch [275/700], Loss: 0.7213, Accuracy: 84.25%\n",
      "Epoch [276/700], Loss: 0.5452, Accuracy: 87.08%\n",
      "Epoch [277/700], Loss: 0.6045, Accuracy: 86.73%\n",
      "Epoch [278/700], Loss: 0.6738, Accuracy: 85.13%\n",
      "Epoch [279/700], Loss: 0.5664, Accuracy: 86.37%\n",
      "Epoch [280/700], Loss: 0.5644, Accuracy: 86.37%\n",
      "Epoch [281/700], Loss: 0.6410, Accuracy: 86.37%\n",
      "Epoch [282/700], Loss: 0.5267, Accuracy: 87.08%\n",
      "Epoch [283/700], Loss: 0.5741, Accuracy: 86.90%\n",
      "Epoch [284/700], Loss: 0.5936, Accuracy: 86.73%\n",
      "Epoch [285/700], Loss: 0.5810, Accuracy: 87.43%\n",
      "Epoch [286/700], Loss: 0.6420, Accuracy: 86.37%\n",
      "Epoch [287/700], Loss: 0.5627, Accuracy: 86.37%\n",
      "Epoch [288/700], Loss: 0.6921, Accuracy: 86.02%\n",
      "Epoch [289/700], Loss: 0.6854, Accuracy: 85.49%\n",
      "Epoch [290/700], Loss: 0.5824, Accuracy: 85.66%\n",
      "Epoch [291/700], Loss: 0.6970, Accuracy: 85.31%\n",
      "Epoch [292/700], Loss: 0.6640, Accuracy: 86.73%\n",
      "Epoch [293/700], Loss: 0.5602, Accuracy: 86.02%\n",
      "Epoch [294/700], Loss: 0.6543, Accuracy: 84.78%\n",
      "Epoch [295/700], Loss: 0.6995, Accuracy: 86.02%\n",
      "Epoch [296/700], Loss: 0.5683, Accuracy: 86.19%\n",
      "Epoch [297/700], Loss: 0.6931, Accuracy: 84.07%\n",
      "Epoch [298/700], Loss: 0.5913, Accuracy: 87.26%\n",
      "Epoch [299/700], Loss: 0.5856, Accuracy: 85.66%\n",
      "Epoch [300/700], Loss: 0.6339, Accuracy: 84.96%\n",
      "Epoch [301/700], Loss: 0.5452, Accuracy: 86.02%\n",
      "Epoch [302/700], Loss: 0.7084, Accuracy: 84.96%\n",
      "Epoch [303/700], Loss: 0.6070, Accuracy: 86.73%\n",
      "Epoch [304/700], Loss: 0.6636, Accuracy: 86.37%\n",
      "Epoch [305/700], Loss: 0.6698, Accuracy: 86.37%\n",
      "Epoch [306/700], Loss: 0.5369, Accuracy: 87.26%\n",
      "Epoch [307/700], Loss: 0.5505, Accuracy: 87.79%\n",
      "Epoch [308/700], Loss: 0.5712, Accuracy: 87.79%\n",
      "Epoch [309/700], Loss: 0.6959, Accuracy: 84.25%\n",
      "Epoch [310/700], Loss: 0.5818, Accuracy: 85.84%\n",
      "Epoch [311/700], Loss: 0.5496, Accuracy: 86.90%\n",
      "Epoch [312/700], Loss: 0.6467, Accuracy: 86.37%\n",
      "Epoch [313/700], Loss: 0.7442, Accuracy: 84.60%\n",
      "Epoch [314/700], Loss: 0.5688, Accuracy: 87.61%\n",
      "Epoch [315/700], Loss: 0.7194, Accuracy: 85.31%\n",
      "Epoch [316/700], Loss: 0.6843, Accuracy: 85.31%\n",
      "Epoch [317/700], Loss: 0.5384, Accuracy: 88.14%\n",
      "Epoch [318/700], Loss: 0.6909, Accuracy: 85.49%\n",
      "Epoch [319/700], Loss: 0.7374, Accuracy: 85.13%\n",
      "Epoch [320/700], Loss: 0.7245, Accuracy: 84.96%\n",
      "Epoch [321/700], Loss: 0.7201, Accuracy: 84.96%\n",
      "Epoch [322/700], Loss: 0.7146, Accuracy: 84.96%\n",
      "Epoch [323/700], Loss: 0.7123, Accuracy: 84.96%\n",
      "Epoch [324/700], Loss: 0.7112, Accuracy: 84.96%\n",
      "Epoch [325/700], Loss: 0.7105, Accuracy: 84.96%\n",
      "Epoch [326/700], Loss: 0.7104, Accuracy: 84.96%\n",
      "Epoch [327/700], Loss: 0.7108, Accuracy: 84.96%\n",
      "Epoch [328/700], Loss: 0.7123, Accuracy: 84.96%\n",
      "Epoch [329/700], Loss: 0.7106, Accuracy: 84.96%\n",
      "Epoch [330/700], Loss: 0.7120, Accuracy: 84.96%\n",
      "Epoch [331/700], Loss: 0.7104, Accuracy: 84.78%\n",
      "Epoch [332/700], Loss: 0.7134, Accuracy: 84.96%\n",
      "Epoch [333/700], Loss: 0.7086, Accuracy: 84.96%\n",
      "Epoch [334/700], Loss: 0.7079, Accuracy: 84.96%\n",
      "Epoch [335/700], Loss: 0.7076, Accuracy: 84.96%\n",
      "Epoch [336/700], Loss: 0.7074, Accuracy: 84.96%\n",
      "Epoch [337/700], Loss: 0.7068, Accuracy: 84.96%\n",
      "Epoch [338/700], Loss: 0.7066, Accuracy: 84.96%\n",
      "Epoch [339/700], Loss: 0.7063, Accuracy: 84.96%\n",
      "Epoch [340/700], Loss: 0.7151, Accuracy: 84.96%\n",
      "Epoch [341/700], Loss: 0.6929, Accuracy: 84.60%\n",
      "Epoch [342/700], Loss: 0.6875, Accuracy: 85.13%\n",
      "Epoch [343/700], Loss: 0.7076, Accuracy: 84.96%\n",
      "Epoch [344/700], Loss: 0.7115, Accuracy: 84.96%\n",
      "Epoch [345/700], Loss: 0.7084, Accuracy: 84.96%\n",
      "Epoch [346/700], Loss: 0.7077, Accuracy: 84.96%\n",
      "Epoch [347/700], Loss: 0.7070, Accuracy: 84.96%\n",
      "Epoch [348/700], Loss: 0.7068, Accuracy: 84.96%\n",
      "Epoch [349/700], Loss: 0.7066, Accuracy: 84.96%\n",
      "Epoch [350/700], Loss: 0.7063, Accuracy: 84.96%\n",
      "Epoch [351/700], Loss: 0.7062, Accuracy: 84.96%\n",
      "Epoch [352/700], Loss: 0.7061, Accuracy: 84.96%\n",
      "Epoch [353/700], Loss: 0.6941, Accuracy: 85.49%\n",
      "Epoch [354/700], Loss: 0.7297, Accuracy: 84.60%\n",
      "Epoch [355/700], Loss: 0.7051, Accuracy: 84.96%\n",
      "Epoch [356/700], Loss: 0.7058, Accuracy: 84.96%\n",
      "Epoch [357/700], Loss: 0.7060, Accuracy: 84.96%\n",
      "Epoch [358/700], Loss: 0.6993, Accuracy: 84.96%\n",
      "Epoch [359/700], Loss: 0.6523, Accuracy: 85.13%\n",
      "Epoch [360/700], Loss: 0.7083, Accuracy: 84.78%\n",
      "Epoch [361/700], Loss: 0.6688, Accuracy: 84.78%\n",
      "Epoch [362/700], Loss: 0.6879, Accuracy: 84.78%\n",
      "Epoch [363/700], Loss: 0.6606, Accuracy: 85.13%\n",
      "Epoch [364/700], Loss: 0.6949, Accuracy: 84.96%\n",
      "Epoch [365/700], Loss: 0.6577, Accuracy: 85.31%\n",
      "Epoch [366/700], Loss: 0.6845, Accuracy: 85.13%\n",
      "Epoch [367/700], Loss: 0.6654, Accuracy: 85.31%\n",
      "Epoch [368/700], Loss: 0.6691, Accuracy: 85.13%\n",
      "Epoch [369/700], Loss: 0.6846, Accuracy: 84.96%\n",
      "Epoch [370/700], Loss: 0.6560, Accuracy: 85.31%\n",
      "Epoch [371/700], Loss: 0.6792, Accuracy: 84.96%\n",
      "Epoch [372/700], Loss: 0.6785, Accuracy: 84.96%\n",
      "Epoch [373/700], Loss: 0.6308, Accuracy: 86.02%\n",
      "Epoch [374/700], Loss: 0.6169, Accuracy: 86.02%\n",
      "Epoch [375/700], Loss: 0.6396, Accuracy: 86.19%\n",
      "Epoch [376/700], Loss: 0.7605, Accuracy: 84.96%\n",
      "Epoch [377/700], Loss: 0.7515, Accuracy: 83.19%\n",
      "Epoch [378/700], Loss: 0.6701, Accuracy: 85.31%\n",
      "Epoch [379/700], Loss: 0.7119, Accuracy: 84.60%\n",
      "Epoch [380/700], Loss: 0.6367, Accuracy: 85.13%\n",
      "Epoch [381/700], Loss: 0.6685, Accuracy: 85.13%\n",
      "Epoch [382/700], Loss: 0.6808, Accuracy: 85.49%\n",
      "Epoch [383/700], Loss: 0.6398, Accuracy: 85.84%\n",
      "Epoch [384/700], Loss: 0.7545, Accuracy: 84.60%\n",
      "Epoch [385/700], Loss: 0.7174, Accuracy: 85.13%\n",
      "Epoch [386/700], Loss: 0.5984, Accuracy: 87.26%\n",
      "Epoch [387/700], Loss: 0.6011, Accuracy: 86.02%\n",
      "Epoch [388/700], Loss: 0.7140, Accuracy: 86.19%\n",
      "Epoch [389/700], Loss: 0.6163, Accuracy: 85.84%\n",
      "Epoch [390/700], Loss: 0.6037, Accuracy: 85.66%\n",
      "Epoch [391/700], Loss: 0.6906, Accuracy: 84.60%\n",
      "Epoch [392/700], Loss: 0.6145, Accuracy: 85.84%\n",
      "Epoch [393/700], Loss: 0.5984, Accuracy: 86.90%\n",
      "Epoch [394/700], Loss: 0.7263, Accuracy: 84.96%\n",
      "Epoch [395/700], Loss: 0.7465, Accuracy: 84.42%\n",
      "Epoch [396/700], Loss: 0.7050, Accuracy: 84.42%\n",
      "Epoch [397/700], Loss: 0.6879, Accuracy: 84.42%\n",
      "Epoch [398/700], Loss: 0.6531, Accuracy: 84.96%\n",
      "Epoch [399/700], Loss: 0.5755, Accuracy: 86.37%\n",
      "Epoch [400/700], Loss: 0.7545, Accuracy: 84.42%\n",
      "Epoch [401/700], Loss: 0.5831, Accuracy: 86.02%\n",
      "Epoch [402/700], Loss: 0.7356, Accuracy: 84.78%\n",
      "Epoch [403/700], Loss: 0.5911, Accuracy: 86.02%\n",
      "Epoch [404/700], Loss: 0.7234, Accuracy: 84.60%\n",
      "Epoch [405/700], Loss: 0.5873, Accuracy: 85.84%\n",
      "Epoch [406/700], Loss: 0.7348, Accuracy: 84.78%\n",
      "Epoch [407/700], Loss: 0.5965, Accuracy: 86.02%\n",
      "Epoch [408/700], Loss: 0.7066, Accuracy: 84.96%\n",
      "Epoch [409/700], Loss: 0.7247, Accuracy: 83.01%\n",
      "Epoch [410/700], Loss: 0.7211, Accuracy: 85.31%\n",
      "Epoch [411/700], Loss: 0.7215, Accuracy: 84.96%\n",
      "Epoch [412/700], Loss: 0.7158, Accuracy: 84.96%\n",
      "Epoch [413/700], Loss: 0.7154, Accuracy: 84.96%\n",
      "Epoch [414/700], Loss: 0.7139, Accuracy: 84.96%\n",
      "Epoch [415/700], Loss: 0.7113, Accuracy: 84.96%\n",
      "Epoch [416/700], Loss: 0.7126, Accuracy: 84.96%\n",
      "Epoch [417/700], Loss: 0.7129, Accuracy: 84.78%\n",
      "Epoch [418/700], Loss: 0.7138, Accuracy: 84.78%\n",
      "Epoch [419/700], Loss: 0.7122, Accuracy: 84.60%\n",
      "Epoch [420/700], Loss: 0.7100, Accuracy: 84.78%\n",
      "Epoch [421/700], Loss: 0.7241, Accuracy: 84.42%\n",
      "Epoch [422/700], Loss: 0.7024, Accuracy: 84.78%\n",
      "Epoch [423/700], Loss: 0.7024, Accuracy: 84.78%\n",
      "Epoch [424/700], Loss: 0.7016, Accuracy: 84.42%\n",
      "Epoch [425/700], Loss: 0.6999, Accuracy: 84.42%\n",
      "Epoch [426/700], Loss: 0.6994, Accuracy: 84.42%\n",
      "Epoch [427/700], Loss: 0.6943, Accuracy: 84.42%\n",
      "Epoch [428/700], Loss: 0.6720, Accuracy: 84.78%\n",
      "Epoch [429/700], Loss: 0.6776, Accuracy: 85.31%\n",
      "Epoch [430/700], Loss: 0.6232, Accuracy: 86.02%\n",
      "Epoch [431/700], Loss: 0.7214, Accuracy: 84.60%\n",
      "Epoch [432/700], Loss: 0.6671, Accuracy: 85.31%\n",
      "Epoch [433/700], Loss: 0.6949, Accuracy: 85.31%\n",
      "Epoch [434/700], Loss: 0.6659, Accuracy: 85.66%\n",
      "Epoch [435/700], Loss: 0.6040, Accuracy: 86.55%\n",
      "Epoch [436/700], Loss: 0.6432, Accuracy: 86.37%\n",
      "Epoch [437/700], Loss: 0.6552, Accuracy: 87.61%\n",
      "Epoch [438/700], Loss: 0.4692, Accuracy: 88.32%\n",
      "Epoch [439/700], Loss: 0.6961, Accuracy: 84.78%\n",
      "Epoch [440/700], Loss: 0.5809, Accuracy: 86.37%\n",
      "Epoch [441/700], Loss: 0.6218, Accuracy: 86.37%\n",
      "Epoch [442/700], Loss: 0.6863, Accuracy: 85.84%\n",
      "Epoch [443/700], Loss: 0.5956, Accuracy: 86.02%\n",
      "Epoch [444/700], Loss: 0.6714, Accuracy: 84.78%\n",
      "Epoch [445/700], Loss: 0.6719, Accuracy: 85.66%\n",
      "Epoch [446/700], Loss: 0.6493, Accuracy: 85.66%\n",
      "Epoch [447/700], Loss: 0.6733, Accuracy: 84.96%\n",
      "Epoch [448/700], Loss: 0.5936, Accuracy: 86.73%\n",
      "Epoch [449/700], Loss: 0.6825, Accuracy: 84.96%\n",
      "Epoch [450/700], Loss: 0.6478, Accuracy: 85.49%\n",
      "Epoch [451/700], Loss: 0.7206, Accuracy: 84.96%\n",
      "Epoch [452/700], Loss: 0.6732, Accuracy: 84.96%\n",
      "Epoch [453/700], Loss: 0.6577, Accuracy: 84.96%\n",
      "Epoch [454/700], Loss: 0.5760, Accuracy: 86.19%\n",
      "Epoch [455/700], Loss: 0.7129, Accuracy: 85.31%\n",
      "Epoch [456/700], Loss: 0.6799, Accuracy: 85.13%\n",
      "Epoch [457/700], Loss: 0.6331, Accuracy: 85.31%\n",
      "Epoch [458/700], Loss: 0.6280, Accuracy: 85.84%\n",
      "Epoch [459/700], Loss: 0.6796, Accuracy: 85.13%\n",
      "Epoch [460/700], Loss: 0.6419, Accuracy: 85.49%\n",
      "Epoch [461/700], Loss: 0.6873, Accuracy: 85.13%\n",
      "Epoch [462/700], Loss: 0.7036, Accuracy: 84.60%\n",
      "Epoch [463/700], Loss: 0.6689, Accuracy: 84.78%\n",
      "Epoch [464/700], Loss: 0.5840, Accuracy: 86.55%\n",
      "Epoch [465/700], Loss: 0.7723, Accuracy: 84.42%\n",
      "Epoch [466/700], Loss: 0.6737, Accuracy: 84.07%\n",
      "Epoch [467/700], Loss: 0.5979, Accuracy: 86.19%\n",
      "Epoch [468/700], Loss: 0.6929, Accuracy: 84.96%\n",
      "Epoch [469/700], Loss: 0.6744, Accuracy: 85.13%\n",
      "Epoch [470/700], Loss: 0.5778, Accuracy: 87.08%\n",
      "Epoch [471/700], Loss: 0.7407, Accuracy: 84.96%\n",
      "Epoch [472/700], Loss: 0.6732, Accuracy: 85.13%\n",
      "Epoch [473/700], Loss: 0.7694, Accuracy: 84.60%\n",
      "Epoch [474/700], Loss: 0.7374, Accuracy: 84.96%\n",
      "Epoch [475/700], Loss: 0.6811, Accuracy: 85.31%\n",
      "Epoch [476/700], Loss: 0.6973, Accuracy: 84.78%\n",
      "Epoch [477/700], Loss: 0.6180, Accuracy: 86.02%\n",
      "Epoch [478/700], Loss: 0.6796, Accuracy: 85.66%\n",
      "Epoch [479/700], Loss: 0.5807, Accuracy: 86.19%\n",
      "Epoch [480/700], Loss: 0.6957, Accuracy: 85.66%\n",
      "Epoch [481/700], Loss: 0.6238, Accuracy: 86.55%\n",
      "Epoch [482/700], Loss: 0.7725, Accuracy: 84.60%\n",
      "Epoch [483/700], Loss: 0.7126, Accuracy: 84.42%\n",
      "Epoch [484/700], Loss: 0.7392, Accuracy: 84.42%\n",
      "Epoch [485/700], Loss: 0.6685, Accuracy: 85.66%\n",
      "Epoch [486/700], Loss: 0.7291, Accuracy: 83.89%\n",
      "Epoch [487/700], Loss: 0.6169, Accuracy: 85.49%\n",
      "Epoch [488/700], Loss: 0.6875, Accuracy: 84.42%\n",
      "Epoch [489/700], Loss: 0.6028, Accuracy: 85.66%\n",
      "Epoch [490/700], Loss: 0.7300, Accuracy: 84.07%\n",
      "Epoch [491/700], Loss: 0.5308, Accuracy: 87.96%\n",
      "Epoch [492/700], Loss: 0.6107, Accuracy: 86.73%\n",
      "Epoch [493/700], Loss: 0.6452, Accuracy: 84.78%\n",
      "Epoch [494/700], Loss: 0.6647, Accuracy: 86.19%\n",
      "Epoch [495/700], Loss: 0.6169, Accuracy: 86.19%\n",
      "Epoch [496/700], Loss: 0.6867, Accuracy: 84.07%\n",
      "Epoch [497/700], Loss: 0.5908, Accuracy: 87.26%\n",
      "Epoch [498/700], Loss: 0.7023, Accuracy: 85.66%\n",
      "Epoch [499/700], Loss: 0.6265, Accuracy: 86.37%\n",
      "Epoch [500/700], Loss: 0.6736, Accuracy: 86.19%\n",
      "Epoch [501/700], Loss: 0.6017, Accuracy: 86.19%\n",
      "Epoch [502/700], Loss: 0.5712, Accuracy: 87.43%\n",
      "Epoch [503/700], Loss: 0.5443, Accuracy: 86.19%\n",
      "Epoch [504/700], Loss: 0.7874, Accuracy: 80.71%\n",
      "Epoch [505/700], Loss: 0.6185, Accuracy: 84.96%\n",
      "Epoch [506/700], Loss: 0.6085, Accuracy: 86.73%\n",
      "Epoch [507/700], Loss: 0.5894, Accuracy: 86.73%\n",
      "Epoch [508/700], Loss: 0.6603, Accuracy: 86.55%\n",
      "Epoch [509/700], Loss: 0.5610, Accuracy: 87.26%\n",
      "Epoch [510/700], Loss: 0.6972, Accuracy: 85.31%\n",
      "Epoch [511/700], Loss: 0.5934, Accuracy: 84.96%\n",
      "Epoch [512/700], Loss: 0.5314, Accuracy: 87.43%\n",
      "Epoch [513/700], Loss: 0.6395, Accuracy: 85.31%\n",
      "Epoch [514/700], Loss: 0.5565, Accuracy: 86.55%\n",
      "Epoch [515/700], Loss: 0.5921, Accuracy: 86.19%\n",
      "Epoch [516/700], Loss: 0.5350, Accuracy: 87.61%\n",
      "Epoch [517/700], Loss: 0.6091, Accuracy: 86.19%\n",
      "Epoch [518/700], Loss: 0.5262, Accuracy: 85.84%\n",
      "Epoch [519/700], Loss: 0.6568, Accuracy: 84.78%\n",
      "Epoch [520/700], Loss: 0.6118, Accuracy: 84.78%\n",
      "Epoch [521/700], Loss: 0.6098, Accuracy: 84.96%\n",
      "Epoch [522/700], Loss: 0.6303, Accuracy: 85.31%\n",
      "Epoch [523/700], Loss: 0.7362, Accuracy: 85.13%\n",
      "Epoch [524/700], Loss: 0.5713, Accuracy: 86.55%\n",
      "Epoch [525/700], Loss: 0.6183, Accuracy: 86.73%\n",
      "Epoch [526/700], Loss: 0.5503, Accuracy: 88.14%\n",
      "Epoch [527/700], Loss: 0.6664, Accuracy: 85.66%\n",
      "Epoch [528/700], Loss: 0.6387, Accuracy: 85.84%\n",
      "Epoch [529/700], Loss: 0.6716, Accuracy: 85.49%\n",
      "Epoch [530/700], Loss: 0.6232, Accuracy: 85.13%\n",
      "Epoch [531/700], Loss: 0.5384, Accuracy: 87.08%\n",
      "Epoch [532/700], Loss: 0.7171, Accuracy: 85.13%\n",
      "Epoch [533/700], Loss: 0.5175, Accuracy: 86.55%\n",
      "Epoch [534/700], Loss: 0.5531, Accuracy: 87.26%\n",
      "Epoch [535/700], Loss: 0.6673, Accuracy: 85.13%\n",
      "Epoch [536/700], Loss: 0.5894, Accuracy: 86.19%\n",
      "Epoch [537/700], Loss: 0.5207, Accuracy: 85.66%\n",
      "Epoch [538/700], Loss: 0.6248, Accuracy: 84.78%\n",
      "Epoch [539/700], Loss: 0.5934, Accuracy: 86.19%\n",
      "Epoch [540/700], Loss: 0.4888, Accuracy: 88.14%\n",
      "Epoch [541/700], Loss: 0.6707, Accuracy: 85.31%\n",
      "Epoch [542/700], Loss: 0.5272, Accuracy: 87.26%\n",
      "Epoch [543/700], Loss: 0.5228, Accuracy: 87.61%\n",
      "Epoch [544/700], Loss: 0.5730, Accuracy: 87.43%\n",
      "Epoch [545/700], Loss: 0.5239, Accuracy: 86.73%\n",
      "Epoch [546/700], Loss: 0.7383, Accuracy: 84.42%\n",
      "Epoch [547/700], Loss: 0.6701, Accuracy: 84.60%\n",
      "Epoch [548/700], Loss: 0.6783, Accuracy: 85.49%\n",
      "Epoch [549/700], Loss: 0.5441, Accuracy: 87.26%\n",
      "Epoch [550/700], Loss: 0.7216, Accuracy: 84.25%\n",
      "Epoch [551/700], Loss: 0.5812, Accuracy: 85.31%\n",
      "Epoch [552/700], Loss: 0.6267, Accuracy: 84.07%\n",
      "Epoch [553/700], Loss: 0.6699, Accuracy: 84.78%\n",
      "Epoch [554/700], Loss: 0.6707, Accuracy: 83.89%\n",
      "Epoch [555/700], Loss: 0.6462, Accuracy: 83.01%\n",
      "Epoch [556/700], Loss: 0.5633, Accuracy: 86.55%\n",
      "Epoch [557/700], Loss: 0.6118, Accuracy: 85.49%\n",
      "Epoch [558/700], Loss: 0.6527, Accuracy: 84.42%\n",
      "Epoch [559/700], Loss: 0.5708, Accuracy: 86.19%\n",
      "Epoch [560/700], Loss: 0.6276, Accuracy: 84.60%\n",
      "Epoch [561/700], Loss: 0.5728, Accuracy: 86.02%\n",
      "Epoch [562/700], Loss: 0.6167, Accuracy: 83.89%\n",
      "Epoch [563/700], Loss: 0.5988, Accuracy: 85.31%\n",
      "Epoch [564/700], Loss: 0.5789, Accuracy: 86.37%\n",
      "Epoch [565/700], Loss: 0.6669, Accuracy: 83.54%\n",
      "Epoch [566/700], Loss: 0.6240, Accuracy: 84.96%\n",
      "Epoch [567/700], Loss: 0.5144, Accuracy: 87.43%\n",
      "Epoch [568/700], Loss: 0.5692, Accuracy: 87.08%\n",
      "Epoch [569/700], Loss: 0.5475, Accuracy: 86.73%\n",
      "Epoch [570/700], Loss: 0.7472, Accuracy: 83.89%\n",
      "Epoch [571/700], Loss: 0.5538, Accuracy: 87.08%\n",
      "Epoch [572/700], Loss: 0.7362, Accuracy: 86.37%\n",
      "Epoch [573/700], Loss: 0.5835, Accuracy: 85.49%\n",
      "Epoch [574/700], Loss: 0.5137, Accuracy: 87.26%\n",
      "Epoch [575/700], Loss: 0.5968, Accuracy: 86.37%\n",
      "Epoch [576/700], Loss: 0.6289, Accuracy: 87.08%\n",
      "Epoch [577/700], Loss: 0.7468, Accuracy: 84.96%\n",
      "Epoch [578/700], Loss: 0.6021, Accuracy: 84.78%\n",
      "Epoch [579/700], Loss: 0.5579, Accuracy: 85.66%\n",
      "Epoch [580/700], Loss: 0.5921, Accuracy: 84.60%\n",
      "Epoch [581/700], Loss: 0.5533, Accuracy: 85.31%\n",
      "Epoch [582/700], Loss: 0.6315, Accuracy: 84.07%\n",
      "Epoch [583/700], Loss: 0.4567, Accuracy: 88.14%\n",
      "Epoch [584/700], Loss: 0.8002, Accuracy: 83.54%\n",
      "Epoch [585/700], Loss: 0.5741, Accuracy: 86.02%\n",
      "Epoch [586/700], Loss: 0.4845, Accuracy: 87.08%\n",
      "Epoch [587/700], Loss: 0.7663, Accuracy: 82.48%\n",
      "Epoch [588/700], Loss: 0.6783, Accuracy: 82.48%\n",
      "Epoch [589/700], Loss: 0.6425, Accuracy: 83.54%\n",
      "Epoch [590/700], Loss: 0.5781, Accuracy: 86.55%\n",
      "Epoch [591/700], Loss: 0.6179, Accuracy: 86.73%\n",
      "Epoch [592/700], Loss: 0.6772, Accuracy: 86.02%\n",
      "Epoch [593/700], Loss: 0.4908, Accuracy: 88.32%\n",
      "Epoch [594/700], Loss: 0.5053, Accuracy: 87.43%\n",
      "Epoch [595/700], Loss: 0.7517, Accuracy: 83.36%\n",
      "Epoch [596/700], Loss: 0.4985, Accuracy: 88.14%\n",
      "Epoch [597/700], Loss: 0.5291, Accuracy: 88.14%\n",
      "Epoch [598/700], Loss: 0.5752, Accuracy: 85.49%\n",
      "Epoch [599/700], Loss: 0.4295, Accuracy: 89.56%\n",
      "Epoch [600/700], Loss: 0.7443, Accuracy: 86.19%\n",
      "Epoch [601/700], Loss: 0.6116, Accuracy: 86.90%\n",
      "Epoch [602/700], Loss: 0.6249, Accuracy: 86.90%\n",
      "Epoch [603/700], Loss: 0.8033, Accuracy: 81.95%\n",
      "Epoch [604/700], Loss: 0.6354, Accuracy: 86.19%\n",
      "Epoch [605/700], Loss: 0.6773, Accuracy: 85.84%\n",
      "Epoch [606/700], Loss: 0.6075, Accuracy: 85.13%\n",
      "Epoch [607/700], Loss: 0.6765, Accuracy: 85.84%\n",
      "Epoch [608/700], Loss: 0.5791, Accuracy: 86.73%\n",
      "Epoch [609/700], Loss: 0.6816, Accuracy: 84.25%\n",
      "Epoch [610/700], Loss: 0.5033, Accuracy: 87.61%\n",
      "Epoch [611/700], Loss: 0.7404, Accuracy: 83.89%\n",
      "Epoch [612/700], Loss: 0.5684, Accuracy: 85.84%\n",
      "Epoch [613/700], Loss: 0.6091, Accuracy: 87.08%\n",
      "Epoch [614/700], Loss: 0.6747, Accuracy: 86.90%\n",
      "Epoch [615/700], Loss: 0.5972, Accuracy: 86.73%\n",
      "Epoch [616/700], Loss: 0.5703, Accuracy: 87.43%\n",
      "Epoch [617/700], Loss: 0.7169, Accuracy: 85.31%\n",
      "Epoch [618/700], Loss: 0.6789, Accuracy: 85.66%\n",
      "Epoch [619/700], Loss: 0.6024, Accuracy: 86.19%\n",
      "Epoch [620/700], Loss: 0.6356, Accuracy: 86.37%\n",
      "Epoch [621/700], Loss: 0.6060, Accuracy: 86.19%\n",
      "Epoch [622/700], Loss: 0.6056, Accuracy: 86.90%\n",
      "Epoch [623/700], Loss: 0.6971, Accuracy: 85.84%\n",
      "Epoch [624/700], Loss: 0.5862, Accuracy: 86.90%\n",
      "Epoch [625/700], Loss: 0.6666, Accuracy: 85.84%\n",
      "Epoch [626/700], Loss: 0.6810, Accuracy: 85.13%\n",
      "Epoch [627/700], Loss: 0.6023, Accuracy: 85.84%\n",
      "Epoch [628/700], Loss: 0.6578, Accuracy: 85.49%\n",
      "Epoch [629/700], Loss: 0.5734, Accuracy: 86.90%\n",
      "Epoch [630/700], Loss: 0.6394, Accuracy: 85.84%\n",
      "Epoch [631/700], Loss: 0.5129, Accuracy: 88.50%\n",
      "Epoch [632/700], Loss: 0.8369, Accuracy: 84.07%\n",
      "Epoch [633/700], Loss: 0.5959, Accuracy: 86.55%\n",
      "Epoch [634/700], Loss: 0.5609, Accuracy: 86.73%\n",
      "Epoch [635/700], Loss: 0.7156, Accuracy: 85.66%\n",
      "Epoch [636/700], Loss: 0.6766, Accuracy: 85.66%\n",
      "Epoch [637/700], Loss: 0.6003, Accuracy: 87.08%\n",
      "Epoch [638/700], Loss: 0.7578, Accuracy: 84.78%\n",
      "Epoch [639/700], Loss: 0.7038, Accuracy: 84.78%\n",
      "Epoch [640/700], Loss: 0.5819, Accuracy: 86.37%\n",
      "Epoch [641/700], Loss: 0.6098, Accuracy: 85.66%\n",
      "Epoch [642/700], Loss: 0.6942, Accuracy: 85.49%\n",
      "Epoch [643/700], Loss: 0.7236, Accuracy: 84.96%\n",
      "Epoch [644/700], Loss: 0.7202, Accuracy: 84.96%\n",
      "Epoch [645/700], Loss: 0.7225, Accuracy: 84.60%\n",
      "Epoch [646/700], Loss: 0.5937, Accuracy: 86.55%\n",
      "Epoch [647/700], Loss: 0.6314, Accuracy: 86.37%\n",
      "Epoch [648/700], Loss: 0.7282, Accuracy: 85.84%\n",
      "Epoch [649/700], Loss: 0.7032, Accuracy: 85.49%\n",
      "Epoch [650/700], Loss: 0.7391, Accuracy: 84.78%\n",
      "Epoch [651/700], Loss: 0.7153, Accuracy: 84.96%\n",
      "Epoch [652/700], Loss: 0.6145, Accuracy: 86.73%\n",
      "Epoch [653/700], Loss: 0.5401, Accuracy: 86.90%\n",
      "Epoch [654/700], Loss: 0.7564, Accuracy: 84.96%\n",
      "Epoch [655/700], Loss: 0.6016, Accuracy: 86.90%\n",
      "Epoch [656/700], Loss: 0.6495, Accuracy: 87.08%\n",
      "Epoch [657/700], Loss: 0.7257, Accuracy: 85.13%\n",
      "Epoch [658/700], Loss: 0.7189, Accuracy: 84.96%\n",
      "Epoch [659/700], Loss: 0.7240, Accuracy: 84.25%\n",
      "Epoch [660/700], Loss: 0.4625, Accuracy: 89.20%\n",
      "Epoch [661/700], Loss: 0.5548, Accuracy: 86.37%\n",
      "Epoch [662/700], Loss: 0.7019, Accuracy: 85.31%\n",
      "Epoch [663/700], Loss: 0.6760, Accuracy: 85.13%\n",
      "Epoch [664/700], Loss: 0.7587, Accuracy: 84.96%\n",
      "Epoch [665/700], Loss: 0.7335, Accuracy: 84.60%\n",
      "Epoch [666/700], Loss: 0.6946, Accuracy: 84.07%\n",
      "Epoch [667/700], Loss: 0.5847, Accuracy: 86.55%\n",
      "Epoch [668/700], Loss: 0.7089, Accuracy: 85.31%\n",
      "Epoch [669/700], Loss: 0.6038, Accuracy: 84.96%\n",
      "Epoch [670/700], Loss: 0.5507, Accuracy: 87.43%\n",
      "Epoch [671/700], Loss: 0.6299, Accuracy: 86.90%\n",
      "Epoch [672/700], Loss: 0.7071, Accuracy: 84.78%\n",
      "Epoch [673/700], Loss: 0.6579, Accuracy: 84.25%\n",
      "Epoch [674/700], Loss: 0.5807, Accuracy: 86.90%\n",
      "Epoch [675/700], Loss: 0.7103, Accuracy: 86.02%\n",
      "Epoch [676/700], Loss: 0.5555, Accuracy: 87.26%\n",
      "Epoch [677/700], Loss: 0.6573, Accuracy: 86.55%\n",
      "Epoch [678/700], Loss: 0.6825, Accuracy: 84.42%\n",
      "Epoch [679/700], Loss: 0.6945, Accuracy: 82.65%\n",
      "Epoch [680/700], Loss: 0.6745, Accuracy: 84.25%\n",
      "Epoch [681/700], Loss: 0.7383, Accuracy: 82.30%\n",
      "Epoch [682/700], Loss: 0.5916, Accuracy: 87.43%\n",
      "Epoch [683/700], Loss: 0.7236, Accuracy: 85.49%\n",
      "Epoch [684/700], Loss: 0.6530, Accuracy: 85.49%\n",
      "Epoch [685/700], Loss: 0.5482, Accuracy: 87.26%\n",
      "Epoch [686/700], Loss: 0.6777, Accuracy: 86.02%\n",
      "Epoch [687/700], Loss: 0.6120, Accuracy: 87.08%\n",
      "Epoch [688/700], Loss: 0.7667, Accuracy: 83.72%\n",
      "Epoch [689/700], Loss: 0.7452, Accuracy: 85.13%\n",
      "Epoch [690/700], Loss: 0.6353, Accuracy: 86.19%\n",
      "Epoch [691/700], Loss: 0.6647, Accuracy: 84.78%\n",
      "Epoch [692/700], Loss: 0.6388, Accuracy: 86.55%\n",
      "Epoch [693/700], Loss: 0.6362, Accuracy: 86.02%\n",
      "Epoch [694/700], Loss: 0.6242, Accuracy: 86.19%\n",
      "Epoch [695/700], Loss: 0.6181, Accuracy: 86.37%\n",
      "Epoch [696/700], Loss: 0.4743, Accuracy: 88.85%\n",
      "Epoch [697/700], Loss: 0.7241, Accuracy: 84.78%\n",
      "Epoch [698/700], Loss: 0.6877, Accuracy: 85.13%\n",
      "Epoch [699/700], Loss: 0.6816, Accuracy: 85.49%\n",
      "Epoch [700/700], Loss: 0.6355, Accuracy: 86.19%\n",
      "Обучение завершено.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Эмуляция входных данных для обучения\n",
    "# Здесь должны использоваться ваши подготовленные данные\n",
    "# sequences и encoded_types, как указано ранее\n",
    "num_epochs = 700\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model = RNNWithAttentionModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for seq, histone in zip(encoded_sequences, encoded_types):\n",
    "        optimizer.zero_grad()  # Обнуляем градиенты\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(seq.unsqueeze(0))  # добавляем размерность батча\n",
    "        \n",
    "        # Вычисляем потери\n",
    "        loss = criterion(outputs, histone.unsqueeze(0))  # добавляем размерность батча\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Рассчитываем количество правильных предсказаний\n",
    "        _, predicted = torch.max(outputs, 1)  # получаем индексы максимальных значений\n",
    "        total_correct += (predicted == histone.unsqueeze(0)).sum().item()\n",
    "        \n",
    "        # Обновляем общее количество образцов\n",
    "        total_samples += 1  # Каждый вход является одним образцом\n",
    "\n",
    "        # Обратный проход и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(encoded_sequences)\n",
    "    accuracy = total_correct / total_samples * 100  # вычисляем точность в процентах\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Обучение завершено.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d1d2032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный тип гистона: H1\n"
     ]
    }
   ],
   "source": [
    "# Входная последовательность\n",
    "seq_fasta = '''>Pan|XP_003311177.1|HTYPE|HVARIANT \n",
    "\n",
    "MSGRGKQGGKARTKAKTRSSRAGLQFPVGRVHRLLRKGNYAERVGAGAPVYLAAVLEYLT \n",
    "\n",
    "AEILELAGNAARDNKKTRIIPRHLQLAIRNDEELNKLLGKVTIAQGGVLPNIQAVLLPKK \n",
    "\n",
    "TESHHKAKGK'''\n",
    "\n",
    "# Извлекаем последовательность, убирая заголовок и пробелы\n",
    "sequence_lines = seq_fasta.split('\\n')[1:]  # пропускаем заголовок\n",
    "sequence = ''.join(line.strip() for line in sequence_lines if line.strip())\n",
    "\n",
    "# Закодируем последовательность в индексы\n",
    "encoded_sequence = torch.tensor([char_to_idx[c] for c in sequence], dtype=torch.long).unsqueeze(0)  # добавляем размерность батча\n",
    "\n",
    "# Сделаем предсказание\n",
    "model.eval()  # Устанавливаем модель в режим оценки\n",
    "with torch.no_grad():  # Отключаем градиенты\n",
    "    outputs = model(encoded_sequence)\n",
    "    _, predicted = torch.max(outputs, 1)  # получаем индексы максимальных значений\n",
    "\n",
    "# Преобразуем индексы обратно в типы гистонов\n",
    "predicted_histone_type = idx_to_char[predicted.item()]  # получаем тип гистона по индексу\n",
    "\n",
    "print(\"Предсказанный тип гистона:\", predicted_histone_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a1b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
