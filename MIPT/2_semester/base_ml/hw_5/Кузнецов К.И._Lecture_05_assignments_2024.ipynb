{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mzfRWZE7cQd"
      },
      "source": [
        "# **Важно!**\n",
        "\n",
        "Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n",
        "*   Баллы выставляются по принципу выполнено/невыполнено.\n",
        "*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n",
        "\n",
        "**Инструкция:** выполните задания в этом же ноутбуке (места под решения **каждой** задачи обозначаются как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**).\n",
        "\n",
        "**Как отправить задание на проверку:** вам необходимо сохранить решение в данном блокноте и отправить итоговый **файл .IPYNB** на учебной платформе в **стандартную форму сдачи домашнего задания.**\n",
        "\n",
        "**Сроки проверки:** преподаватель проверит домашнее задание в течение недели после дедлайна и даст вам обратную связь.\n",
        "\n",
        "# **Перед выполнением задания**\n",
        "\n",
        "1. **Перезапустите ядро (restart the kernel):** в меню выбрать **Ядро (Kernel)**\n",
        "→ **Перезапустить (Restart).**\n",
        "2. **Выполните** **все ячейки (run all cells)**: в меню выбрать **Ячейка (Cell)**\n",
        "→ **Запустить все (Run All)**.\n",
        "\n",
        "После ячеек с заданием следуют ячейки с проверкой **с помощью assert**: если в коде есть ошибки, assert выведет уведомление об ошибке; если в коде нет ошибок, assert отработает без вывода дополнительной информации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pR8uLgW7qe-"
      },
      "source": [
        "# Домашнее задание 5. Деревья решений. Композиции деревьев. Случайный лес\n",
        "\n",
        "**Цели:**\n",
        "\n",
        "*   Научиться строить дерево решений вручную с помощью рекурсии и применить его к данным Iris.\n",
        "*   Добавить регуляризацию.\n",
        "*   Применить на практике такие методы, как деревья решений, композиции деревьев, случайный лес.\n",
        "\n",
        "# Условия домашней работы\n",
        "\n",
        "**Решающее дерево** — это алгоритм машинного обучения, который используется для решения задач классификации и регрессии. Оно представляет собой древовидную структуру, где каждый узел представляет тест на одном из признаков, а каждая ветвь — возможный результат этого теста. Листья дерева представляют собой конечный результат — прогноз для новых данных.\n",
        "\n",
        "В процессе построения решающего дерева алгоритм выбирает тест, который лучше всего разделяет данные на различные классы или предсказывает значение целевой переменной. Затем данные разбиваются на две или более частей в соответствии с результатами теста. Этот процесс повторяется для каждой полученной части, пока не будет достигнут критерий останова."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R-5jcaGr7b6N"
      },
      "outputs": [],
      "source": [
        "# Запустите эту ячейку для первоначальной настройки\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.rc('lines', linewidth=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ1H2WLM9J3H"
      },
      "source": [
        "## Часть 1. Реализация деревья решений для категориальных данных через прирост информации с энтропией\n",
        "\n",
        "**Задание 1 (1 балл)**\n",
        "\n",
        "Дополните функцию по расчету энтропии `compute_entropy`. Функция должна принимать вектор y с дискретными значениями (list) и вычислять значение энтропии для данного вектора. Для подсчета уникальных значений в векторе значений удобно использовать функцию `np.unique` с аргументом `return_counts=True`. Функция должна возвращать нулевое значение в случае пустого списка или списка с единственным уникальным значением."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PQDcKOwx9Z_j"
      },
      "outputs": [],
      "source": [
        "def compute_entropy(y):\n",
        "    if len(y) == 0:\n",
        "        return 0\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    _, probs = np.unique(y, return_counts=True)\n",
        "    probs = probs/len(y)\n",
        "    entropy = -np.sum(probs*np.log2(probs))\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yefXge8P_9qN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "# Пустой вектор:\n",
        "assert compute_entropy(np.array([])) == 0.\n",
        "# Вектор с одним элементом:\n",
        "assert compute_entropy(np.array([1])) == 0.\n",
        "# Вектор с двумя одинаковыми элементами:\n",
        "assert compute_entropy(np.array([2, 2])) == 0.\n",
        "# Вектор с двумя разными элементами:\n",
        "assert compute_entropy(np.array([1, 2])) == 1.\n",
        "# Вектор с тремя одинаковыми элементами:\n",
        "assert compute_entropy(np.array([0, 0, 0])) == 0.\n",
        "# Вектор с тремя элементами, два из которых одинаковые:\n",
        "assert np.isclose(compute_entropy(np.array([0, 0, 1])), 0.92, atol=0.01)\n",
        "# Вектор с тремя элементами, все элементы разные:\n",
        "assert np.isclose(compute_entropy(np.array([1, 2, 3])), 1.58, atol=0.01)\n",
        "# Вектор с четырьмя одинаковыми элементами:\n",
        "assert compute_entropy(np.array([7, 7, 7, 7])) == 0.\n",
        "# Вектор с четырьмя элементами, два из которых одинаковые:\n",
        "assert compute_entropy(np.array([5, 5, 2, 1])) == 1.5\n",
        "# Вектор с пятью элементами, все элементы разные:\n",
        "assert np.isclose(compute_entropy(np.array([5, 4, 3, 2, 1])), 2.32, atol=0.01)\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YyJ8_9rA265"
      },
      "source": [
        "**Задание 2 (1 балл)**\n",
        "\n",
        "\n",
        "Дополните функцию для прироста информации для задачи классификации. Функция должна принимать три параметра: набор целевых значений `y` и список со списками индексов `subsets`.\n",
        "\n",
        "Для начала вам понадобится рассчитать значение энтропии для данных целевых значений `y`; запишите ее значение в переменную `total_entropy`. Затем рассчитайте сумму взвешенных критериев подгрупп `weighted_entropy`. Изначально она задана равной нулю, затем прибавляем взвешенную энтропию для каждой подгруппы.\n",
        "\n",
        "На последнем этапе рассчитайте разницу между полным значением критерия `total_entropy` и суммой взвешенных критериев подгрупп `weighted_entropy`, `gain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jrre5eg5AQJg"
      },
      "outputs": [],
      "source": [
        "def information_gain_entropy(y, left, right):\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    # Конвертируем в массивы numpy\n",
        "    y = np.array(y)\n",
        "    left = np.array(left)\n",
        "    right = np.array(right)\n",
        "    \n",
        "    # Проверяем не являются ли группы разбиения пустыми\n",
        "    if len(left) == 0 or len(right) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # Рассчитываем общую энтропию и энтропию подгрупп\n",
        "    total_entropy = compute_entropy(y)\n",
        "    left_entropy = (len(left)/len(y))*compute_entropy(left)\n",
        "    right_entropy = (len(right)/len(y))*compute_entropy(right)\n",
        "    \n",
        "    # Рассчитываем прирост информации\n",
        "    gain = total_entropy - left_entropy - right_entropy\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "    return gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QhBo0p0q1ra2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "# ПРОВЕРКА\n",
        "y, left, right = [0, 0, 1, 1, 1, 1], [0, 0], [1, 1, 1, 1]\n",
        "assert np.isclose(information_gain_entropy(y, left, right), 0.92, atol=0.01)\n",
        "\n",
        "y, left, right = [0, 0, 0, 1, 1, 1], [0, 0, 0], [1, 1, 1]\n",
        "assert information_gain_entropy(y, left, right) == 1.0\n",
        "\n",
        "y, left, right = [0, 1, 0, 1], [0, 1], [0, 1]\n",
        "assert information_gain_entropy(y, left, right) == 0.0\n",
        "\n",
        "y, left, right = [0, 1, 1, 0, 0], [0], [1, 1, 0, 0]\n",
        "assert np.isclose(information_gain_entropy(y, left, right), 0.17, atol=0.01)\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dHMDLcMf8l0"
      },
      "source": [
        "**Задание 3 (1 балл)**\n",
        "\n",
        "Дополните функцию расчета наилучшего разбиения узла `get_best_split`. Эта функция вызывается для каждого узла дерева с целью оптимальным образом разбить данные `X` и `y`, дошедшие до данного узла. Для этого мы должны перебрать признаки в `X` и все возможные пороговые значения для каждого признака и выбрать ту комбинацию, которая ведет к максимальному приросту информации.\n",
        "\n",
        "Например, если `X = [[1, 4], [6.5, 7], [9, 4]]`, мы имеем дело с двумя признаками. Первый признак принимает значения `[1, 6.5, 9]`. Мы будем рассматривать пороговые значения 1 и 6.5, рассчитывая  приросты разбиений `[[1], [6.5, 9]]` и `[[1, 6.5], [9]]`. Второй признак принимает значения 4 и 7 и имеет единственный возможный порог 4 и разбиение `[[4, 4], [7]]`.\n",
        "\n",
        "Допустим, что `y = [0, 1, 0]`. Для трех приведенных выше вариантов разбиений по признаку и пороговому значению приросты окажутся равны 0.25, 0.25 и 0.92. Следовательно, в этом узле дерево будет разбивать данные на подгруппы по второму признаку с пороговым значением 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZzrwTxPVANcU"
      },
      "outputs": [],
      "source": [
        "def get_best_split(X, y):\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    best_gain = 0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "\n",
        "    for feature_idx in range(n_features):\n",
        "        feature_values = X[:, feature_idx]\n",
        "        # Находим уникальные значения признака.\n",
        "        thresholds = np.unique(feature_values)\n",
        "        # Итерируемся по всем возможным пороговым значениям признака.\n",
        "        for threshold in thresholds:\n",
        "            # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
        "            left_indices = X[:, feature_idx] <= threshold\n",
        "            right_indices = X[:, feature_idx] > threshold\n",
        "\n",
        "            left_y = y[left_indices]\n",
        "            right_y = y[right_indices]\n",
        "            # Рассчитываем прирост информации.\n",
        "            gain = information_gain_entropy(y, left=left_y, right=right_y,)\n",
        "            # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "            # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature_idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_gain, best_feature, best_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fZrZkUzj1ssT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "# ПРОВЕРКА\n",
        "X = np.array([[2, 3], [1, 4], [2, 6], [3, 3], [2, 5], [1, 6]])\n",
        "y = np.array([0, 0, 1, 1, 1, 0])\n",
        "gain, feature, threshold = get_best_split(X, y)\n",
        "assert np.isclose(gain, 0.46, atol=0.01) and feature == 0 and threshold == 1\n",
        "\n",
        "X = np.array([[1, 1], [1, 1], [1, 1], [1, 1]])\n",
        "y = np.array([0, 0, 0, 0])\n",
        "gain, feature, threshold = get_best_split(X, y)\n",
        "assert gain == 0 and feature is None and threshold is None\n",
        "\n",
        "X = np.array([[2, 3], [1, 4], [3, 6], [4, 3], [2, 5], [3, 7]])\n",
        "y = np.array([0, 0, 1, 1, 1, 1])\n",
        "gain, feature, threshold = get_best_split(X, y)\n",
        "assert np.isclose(gain, 0.46, atol=0.01) and feature == 0 and threshold == 2\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 3], [4, 4], [2, 5], [3, 6]])\n",
        "y = np.array([0, 0, 1, 1, 0, 1])\n",
        "gain, feature, threshold = get_best_split(X, y)\n",
        "assert gain == 1 and feature == 0 and threshold == 2\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 3], [4, 4], [2, 5], [3, 6]])\n",
        "y = np.array([0, 0, 1, 1, 0, 1])\n",
        "gain, feature, threshold = get_best_split(X, y)\n",
        "assert gain == 1 and feature == 0 and threshold == 2\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfBDMFd2Eica"
      },
      "source": [
        "**Задание 4 (1 балл)**\n",
        "\n",
        "Постройте дерево решений, пользуясь ранее созданными функциями и данным кодом.\n",
        "\n",
        "В этой задаче мы будем пользоваться рекурсией, то есть вызовом функцией самой себя. Для задач рекурсии необходимо определить основное действие, которое мы хотим повторять, и базовые случаи, когда рекурсия должна прекратиться. Здесь базовым случаем станет ситуация, когда все значения целевой переменной в разбиении равны. Это условия превращает узел в лист, который и выдает конечное значение `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "joOxOBHTDTZ4"
      },
      "outputs": [],
      "source": [
        "def decision_tree_entropy(X, y):\n",
        "    # Базовый случай\n",
        "    if len(np.unique(y)) == 1:\n",
        "        return {\"label\": y[0]}\n",
        "\n",
        "    # Находим наиболее благоприятное разбиение\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    gain, feature, threshold = get_best_split(X, y)\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "\n",
        "    # В случае нулевого прироста возвращаем наиболее представленный класс\n",
        "    if gain <= 0:\n",
        "        return {\"label\": np.bincount(y).argmax()}\n",
        "\n",
        "    # Разделяем данные в соответствии со значением threshold\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    left_indices = X[:,feature] <= threshold\n",
        "    right_indices = X[:,feature] > threshold\n",
        "    \n",
        "    \n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "\n",
        "    # Строим дерево рекурсивно (функция вызывает саму себя на \"левых\" и \"правых\" данных)\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    left_tree = decision_tree_entropy(X[left_indices], y[left_indices])\n",
        "    right_tree = decision_tree_entropy(X[right_indices], y[right_indices])\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "\n",
        "    # Возвращаем текущий узел\n",
        "    return {\n",
        "        \"feature\": feature,\n",
        "        \"threshold\": threshold,\n",
        "        \"left\": left_tree,\n",
        "        \"right\": right_tree,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UANQe3_b17kB"
      },
      "source": [
        "Обучим наше дерево на данных Iris."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JpBUhPP91-pZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Загрузка данных iris\n",
        "data = load_iris()\n",
        "X = data.data  # Признаки\n",
        "y = data.target  # Целевая переменная\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Строим дерево\n",
        "tree = decision_tree_entropy(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'feature': 2,\n",
              " 'threshold': np.float64(1.7),\n",
              " 'left': {'label': np.int64(0)},\n",
              " 'right': {'feature': 3,\n",
              "  'threshold': np.float64(1.7),\n",
              "  'left': {'feature': 2,\n",
              "   'threshold': np.float64(4.9),\n",
              "   'left': {'feature': 3,\n",
              "    'threshold': np.float64(1.6),\n",
              "    'left': {'label': np.int64(1)},\n",
              "    'right': {'label': np.int64(2)}},\n",
              "   'right': {'feature': 3,\n",
              "    'threshold': np.float64(1.5),\n",
              "    'left': {'label': np.int64(2)},\n",
              "    'right': {'feature': 0,\n",
              "     'threshold': np.float64(6.7),\n",
              "     'left': {'label': np.int64(1)},\n",
              "     'right': {'label': np.int64(2)}}}},\n",
              "  'right': {'feature': 2,\n",
              "   'threshold': np.float64(4.8),\n",
              "   'left': {'feature': 0,\n",
              "    'threshold': np.float64(5.9),\n",
              "    'left': {'label': np.int64(1)},\n",
              "    'right': {'label': np.int64(2)}},\n",
              "   'right': {'label': np.int64(2)}}}}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9hmsiual97qZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "# ПРОВЕРКА\n",
        "assert tree['feature'] == 2\n",
        "assert tree['right']['left']['left']['threshold'] == 1.6\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYatsNRL0QP_"
      },
      "source": [
        "После того как дерево построено с помощью обучающей выборки, мы можем воспользоваться им для предсказания значений из тестовой выборки. Для этого нам нужна предсказательная функция, которая пропустит тестовые данные через дерево. Ее код дан ниже. Обратите внимание, что функция `prediction` тоже рекурсивна."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sm428yqZ9WLw"
      },
      "outputs": [],
      "source": [
        "def prediction(tree, X):\n",
        "    # Проверяем, является ли X одномерным массивом (одна запись)\n",
        "    if X.ndim == 1:\n",
        "        # Если узел - лист, возвращаем его значение\n",
        "        if \"label\" in tree:\n",
        "            return tree[\"label\"]\n",
        "\n",
        "        # В противном случае, рекурсивно спускаемся по дереву\n",
        "        feature = tree[\"feature\"]\n",
        "        threshold = tree[\"threshold\"]\n",
        "\n",
        "        # Если значение признака меньше или равно порогу, идем в левое поддерево, иначе в правое\n",
        "        if X[feature] <= threshold:\n",
        "            return prediction(tree[\"left\"], X)\n",
        "        else:\n",
        "            return prediction(tree[\"right\"], X)\n",
        "    else:\n",
        "        # Если X - матрица, проходим по каждой строке в данных\n",
        "        return np.array([prediction(tree, row) for row in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZiE81k_R7T5G"
      },
      "outputs": [],
      "source": [
        "y_pred = prediction(tree, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HyKfeLN1C8ni"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ПРОВЕРКА\n",
        "assert accuracy_score(y_test, y_pred) > 0.93\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCEGR3eI_Gk0"
      },
      "source": [
        "Поздравляем с первым реализованным деревом решений!\n",
        "\n",
        "Теперь попробуем обновить функции: сменим критерий на индекс Джини и добавим регуляризацию по максимальной глубине дерева."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QliEyE38zbN"
      },
      "source": [
        "**Задание 5 (1 балл)**\n",
        "\n",
        "Реализуйте функцию по расчету индекса Джини. Не забудьте про краевые случаи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eCCCASOu8xzV"
      },
      "outputs": [],
      "source": [
        "def gini_index(y):\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    # Возврат нуля при однородном классе (0-1 элемент в листе)\n",
        "    if len(y) <=1:\n",
        "        return 0\n",
        "    \n",
        "    # По аналогии с энтропией Шеннона расчитываем критерий Джини\n",
        "    _, probs = np.unique(y, return_counts=True)\n",
        "    probs = probs/len(y)\n",
        "    gini = 1 - np.sum(probs**2)\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "    return gini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3z6VFhWTAh3i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "# Пустой вектор:\n",
        "assert gini_index(np.array([])) == 0\n",
        "# Вектор с одним элементом:\n",
        "assert gini_index(np.array([1])) == 0\n",
        "# Вектор с двумя одинаковыми элементами:\n",
        "assert gini_index(np.array([2,2])) == 0\n",
        "# Вектор с двумя разными элементами:\n",
        "assert gini_index(np.array([1,2])) == 0.5\n",
        "# Вектор с тремя одинаковыми элементами:\n",
        "assert gini_index(np.array([0,0,0])) == 0\n",
        "# Вектор с тремя элементами, два из которых одинаковые:\n",
        "assert gini_index(np.array([0,0,1])) == 0.4444444444444444\n",
        "# Вектор с тремя элементами, все элементы разные:\n",
        "assert gini_index(np.array([1,2,3])) == 0.6666666666666667\n",
        "# Вектор с четырьмя одинаковыми элементами:\n",
        "assert gini_index(np.array([7,7,7,7])) == 0\n",
        "# Вектор с четырьмя элементами, два из которых одинаковые:\n",
        "assert gini_index(np.array([5,5,2,1])) == 0.625\n",
        "# Вектор с пятью элементами, все элементы разные:\n",
        "assert np.isclose(gini_index(np.array([5,4,3,2,1])), 0.80, atol=0.01)\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMVRRfD6Dt5H"
      },
      "source": [
        "**Задание 6 (1 балл)**\n",
        "\n",
        "Реализуйте функцию по расчету прироста информации на базе индекса Джини."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nLWlQ3upBYt3"
      },
      "outputs": [],
      "source": [
        "def information_gain_gini(y, left, right):\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    # Конвертируем в массивы numpy\n",
        "    y = np.array(y)\n",
        "    left = np.array(left)\n",
        "    right = np.array(right)\n",
        "\n",
        "    # Проверяем не являются ли группы разбиения пустыми\n",
        "    if len(left) == 0 or len(right) == 0:\n",
        "        return 0\n",
        "\n",
        "    # Рассчитываем общую энтропию и энтропию подгрупп\n",
        "    total_entropy = gini_index(y)\n",
        "    left_entropy = (len(left)/len(y))*gini_index(left)\n",
        "    right_entropy = (len(right)/len(y))*gini_index(right)\n",
        "\n",
        "    # Рассчитываем прирост информации\n",
        "    gain = total_entropy - left_entropy - right_entropy\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "    return gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2z-RyxYbOyyI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "# ПРОВЕРКА\n",
        "y, left, right = [0, 0, 1, 1, 1, 1], [0, 0], [1, 1, 1, 1]\n",
        "assert np.isclose(information_gain_gini(y, left, right), 0.44, atol=0.01)\n",
        "\n",
        "y, left, right = [0, 0, 0, 1, 1, 1], [0, 0, 0], [1, 1, 1]\n",
        "assert information_gain_gini(y, left, right) == 0.5\n",
        "\n",
        "y, left, right = [0, 1, 0, 1], [0, 1], [0, 1]\n",
        "assert information_gain_gini(y, left, right) == 0.0\n",
        "\n",
        "y, left, right = [0, 1, 1, 0, 0], [0], [1, 1, 0, 0]\n",
        "assert np.isclose(information_gain_gini(y, left, right), 0.08, atol=0.01)\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XtU3lunTYkk"
      },
      "source": [
        "**Задание 7 (2 балла)**\n",
        "\n",
        "Дополните функцию `decision_tree` условиями на максимальное число узлов `max_depth`. Для этого нужно вести счет узлам, увеличивая его на единицу при каждом разветвлении. В случае когда достигнута максимальная глубина, функция должна возвращать наиболее представленный класс (выше мы уже пользовались `np.bincount`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Добавим вспомогательную функцию разбиения \n",
        "def get_best_split_gini(X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    best_gain = 0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "\n",
        "    for feature_idx in range(n_features):\n",
        "        feature_values = X[:, feature_idx]\n",
        "        # Находим уникальные значения признака.\n",
        "        thresholds = np.unique(feature_values)\n",
        "        # Итерируемся по всем возможным пороговым значениям признака.\n",
        "        for threshold in thresholds:\n",
        "            # Определяем индексы объектов, которые относятся к левому поддереву и правому поддереву.\n",
        "            left_indices = X[:, feature_idx] <= threshold\n",
        "            right_indices = X[:, feature_idx] > threshold\n",
        "\n",
        "            left_y = y[left_indices]\n",
        "            right_y = y[right_indices]\n",
        "            # Рассчитываем прирост информации.\n",
        "            gain = information_gain_gini(y, left=left_y, right=right_y,)\n",
        "            # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "            # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature_idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_gain, best_feature, best_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uAs_UkOrTYAt"
      },
      "outputs": [],
      "source": [
        "def decision_tree_gini(X, y, depth=0, max_depth=None):\n",
        "    # Базовый случай\n",
        "    if len(np.unique(y)) == 1:\n",
        "        return {\"label\": y[0]}\n",
        "\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    if max_depth is not None and depth >= max_depth:\n",
        "        return {\"label\": np.bincount(y).argmax()}\n",
        "\n",
        "    # Находим наилучшее разбиение\n",
        "    gain, feature, threshold = get_best_split_gini(X, y)\n",
        "\n",
        "    if gain <= 0:\n",
        "        return {\"label\": np.bincount(y).argmax()}\n",
        "\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "\n",
        "    # Допишите остаток функции по аналогии с предыдущей реализацией дерева решений\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    left_indices = X[:, feature] <= threshold\n",
        "    right_indices = X[:, feature] > threshold\n",
        "\n",
        "    left_tree = decision_tree_gini(\n",
        "        X[left_indices], y[left_indices], depth + 1, max_depth)\n",
        "    right_tree = decision_tree_gini(\n",
        "        X[right_indices], y[right_indices], depth + 1, max_depth)\n",
        "\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
        "\n",
        "    # Возвращаем текущий узел\n",
        "    return {\n",
        "        \"feature\": feature,\n",
        "        \"threshold\": threshold,\n",
        "        \"left\": left_tree,\n",
        "        \"right\": right_tree,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Iu3Tz0KTSvo5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "# ПРОВЕРКА\n",
        "tree = decision_tree_gini(X_train, y_train, max_depth=2)\n",
        "assert tree['feature'] == 2\n",
        "assert tree['right']['left']['label'] == 1\n",
        "\n",
        "tree = decision_tree_gini(X_train, y_train, max_depth=1)\n",
        "assert tree['left']['label'] == 0\n",
        "assert tree['right']['label'] == 2\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaJU3goQUqlf"
      },
      "source": [
        "### Часть 2. Использование библиотечных функций. Бэггинг\n",
        "\n",
        "В этой части мы воспользуемся готовыми функциями из библиотеки `sklearn`. Мы обучим и оценим модель на уже знакомом наборе данных Iris."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxDnqHHCMVzM"
      },
      "source": [
        "**Задание 8 (1 балл)**\n",
        "\n",
        "Инициализируйте бэггинг-классификатор на основе класса `BaggingClassifier`, в параметр `estimator` передайте значение `DecisionTreeClassifier()`, с `n_estimators=10`. Не забудьте задать `random_state` равный нулю.\n",
        "Экземпляр сохраните в переменную `bagging`.\n",
        "\n",
        "Обучите модель по аналогии с предыдущими библиотечными моделями от sklearn. Посчитайте предсказание для тестовой выборки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2M-WmtA4MNZM"
      },
      "outputs": [],
      "source": [
        "# Загрузка библиотек\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "tree = DecisionTreeClassifier()\n",
        "bagging = BaggingClassifier(estimator=tree, n_estimators=10, random_state=0)\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4J4MSqAvWMON"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "print(accuracy_score(y_test, y_pred_bagging))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "s1UGZUB17Z-i",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "91b88bf44d770c08dd5b2d59408cbee6",
          "grade": false,
          "grade_id": "cell-7d570636228d9211",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## Часть 3. Случайный лес для регрессии.\n",
        "\n",
        "**Случайный лес (Random Forest)** — это алгоритм машинного обучения, который является комбинацией множества решающих деревьев. Он применяется как для задач классификации, так и для задач регрессии.\n",
        "\n",
        "Суть метода заключается в том, что мы строим несколько деревьев решений на случайных подмножествах данных и случайных подмножествах признаков на каждом нелистовом узле, а затем усредняем их ответы для уменьшения эффекта переобучения. Для каждого дерева в случайном лесу используется только подмножество данных, которое выбирается случайным образом с возвращением (bootstrap).\n",
        "\n",
        "Кроме того, для каждого разбиения дерева в случайном лесу выбирается только подмножество признаков, которые можно использовать для разделения узлов. Это позволяет получить более разнообразные деревья и уменьшить вероятность переобучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "PZ9ZckvI7Z-i",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "541f02d21d69d2a50aab2bfc45975250",
          "grade": false,
          "grade_id": "cell-4b968d3440b77d2c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Загрузка данных\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Загружаем California housing датасет\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Разделим данные на тренировочную и тестовую выборки:\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HBJ9U1wO7Z-k",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3921cf0346aafca5d241cecee8073f46",
          "grade": false,
          "grade_id": "cell-585aaf2bfde505b2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**Задание 9 (1 балл)**\n",
        "\n",
        "Инициализируйте экземпляр класса `RandomForestRegressor`,\n",
        "c `n_estimators` равным 100, и `max_depth` равным 5.\n",
        "Снова задайте параметр `random_state` равным 0.\n",
        "Экземпляр сохраните в переменную `rfr`. Обучите модель и рассчитайте MAE с помощью функции `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "w99eLKH_7Z-k",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "957369930f618c8cbf2eaded60af60a5",
          "grade": false,
          "grade_id": "cell-0807738669e9b21b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "5567e7aa-b21e-417d-bf09-fcefd10f877c"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "rfr = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=0)\n",
        "rfr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rfr.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HJG5qqd37Z-k",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "235482e382bbb541957cf46ef03cf59e",
          "grade": true,
          "grade_id": "cell-31e4b54fb1755366",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тесты пройдены!\n"
          ]
        }
      ],
      "source": [
        "assert type(rfr) == RandomForestRegressor\n",
        "assert mae == 0.4897545742445902\n",
        "\n",
        "print(\"Тесты пройдены!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAMu7REdZsWE"
      },
      "source": [
        "# Поздравляем!\n",
        "\n",
        "В этом домашнем задании вы вручную реализовали дерево решений, познакомились с рекурсией. Вы также освоили две новые библиотеки `sklearn` и готовы применять их для любых задач, будь то классификация или регрессия. Мы надеемся, что теперь вы лучше понимаете механизм работы деревьев!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dlenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
